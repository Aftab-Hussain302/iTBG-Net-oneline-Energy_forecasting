{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":25046,"databundleVersionId":1852010,"sourceType":"competition"},{"sourceId":4597716,"sourceType":"datasetVersion","datasetId":2678471},{"sourceId":10168101,"sourceType":"datasetVersion","datasetId":6264601},{"sourceId":10216981,"sourceType":"datasetVersion","datasetId":6315439}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport zipfile\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# -------------------------------\n# 1. Setup: Create Directory for Plots\n# -------------------------------\nplot_dir = \"/kaggle/working/Dataset_plots\"\nos.makedirs(plot_dir, exist_ok=True)  # Create directory if it does not exist\n\n# -------------------------------\n# 2. Load and Prepare the Dataset\n# -------------------------------\nfile_path = \"/kaggle/input/integrated-energy-management-and-forecasting/Integrated Energy Management and Forecasting Dataset.csv\"\ndf = pd.read_csv(file_path)\n\n# Convert 'Timestamp' column to datetime for time-series analysis\ndf['Timestamp'] = pd.to_datetime(df['Timestamp'])\n\n# List of numerical columns for analysis\nnumerical_cols = ['Energy_Demand', 'Energy_Supply', 'Temperature', 'Grid_Load',\n                  'Renewable_Source_Output', 'NonRenewable_Source_Output', 'Energy_Price']\n\n# Set a common plot style\nsns.set(style=\"whitegrid\")\n\n# -------------------------------\n# 3. Function to Save Plots\n# -------------------------------\ndef save_plot(filename):\n    \"\"\"Save the current plot in multiple formats.\"\"\"\n    for ext in ['png', 'eps', 'pdf']:\n        plt.savefig(os.path.join(plot_dir, f\"{filename}.{ext}\"), format=ext, bbox_inches='tight')\n    plt.close()\n\n# -------------------------------\n# 4. Time Series Analysis\n# -------------------------------\nplt.figure(figsize=(14, 6))\nplt.plot(df['Timestamp'], df['Energy_Demand'], label='Energy Demand', color='blue')\nplt.plot(df['Timestamp'], df['Energy_Supply'], label='Energy Supply', color='green')\nplt.xlabel('Timestamp')\nplt.ylabel('Value')\nplt.title('Time Series of Energy Demand and Energy Supply')\nplt.legend()\nplt.xticks(rotation=45)\nplt.tight_layout()\nsave_plot(\"time_series_energy_demand_supply\")\n\n# -------------------------------\n# 5. Distribution Analysis\n# -------------------------------\ndf[numerical_cols].hist(bins=15, figsize=(15, 10), layout=(3, 3))\nplt.suptitle(\"Histograms of Numerical Features\", fontsize=16)\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])\nsave_plot(\"histograms_numerical_features\")\n\nplt.figure(figsize=(14, 8))\nfor col in numerical_cols:\n    sns.kdeplot(data=df, x=col, fill=True, label=col)\nplt.title('KDE Plot of Numerical Features')\nplt.xlabel('Value')\nplt.legend()\nsave_plot(\"kde_numerical_features\")\n\n# -------------------------------\n# 6. Correlation Analysis\n# -------------------------------\ncorr = df[numerical_cols].corr()\nplt.figure(figsize=(10, 8))\nsns.heatmap(corr, annot=True, cmap='coolwarm', fmt=\".2f\")\nplt.title(\"Correlation Heatmap of Numerical Features\")\nsave_plot(\"correlation_heatmap\")\n\n# -------------------------------\n# 7. Categorical Analysis\n# -------------------------------\nplt.figure(figsize=(8, 4))\nsns.countplot(x='Weather_Condition_x', data=df, palette='Set2')\nplt.title('Count of Observations by Weather_Condition_x')\nplt.xlabel('Weather Condition (x)')\nplt.ylabel('Count')\nsave_plot(\"weather_condition_x_distribution\")\n\nplt.figure(figsize=(8, 4))\nsns.countplot(x='Weather_Condition_y', data=df, palette='Set3')\nplt.title('Count of Observations by Weather_Condition_y')\nplt.xlabel('Weather Condition (y)')\nplt.ylabel('Count')\nsave_plot(\"weather_condition_y_distribution\")\n\nplt.figure(figsize=(14, 6))\nplt.subplot(1, 2, 1)\nsns.boxplot(x='Weather_Condition_x', y='Energy_Demand', data=df, palette='pastel')\nplt.title('Energy Demand by Weather_Condition_x')\nplt.xticks(rotation=45)\n\nplt.subplot(1, 2, 2)\nsns.boxplot(x='Weather_Condition_x', y='Energy_Supply', data=df, palette='pastel')\nplt.title('Energy Supply by Weather_Condition_x')\nplt.xticks(rotation=45)\n\nplt.tight_layout()\nsave_plot(\"energy_demand_supply_by_weather_condition_x\")\n\n# -------------------------------\n# 8. Scatter Plots\n# -------------------------------\nplt.figure(figsize=(10, 6))\nsns.scatterplot(x='Grid_Load', y='Energy_Demand', data=df, hue='Weather_Condition_x', palette='deep')\nplt.title('Grid Load vs. Energy Demand')\nplt.xlabel('Grid Load')\nplt.ylabel('Energy Demand')\nplt.legend(title='Weather Condition')\nsave_plot(\"scatter_grid_load_vs_energy_demand\")\n\nplt.figure(figsize=(10, 6))\nsns.scatterplot(\n    x='Renewable_Source_Output',\n    y='NonRenewable_Source_Output',\n    data=df,\n    hue='Energy_Price',\n    palette='viridis',\n    legend=False\n)\nplt.title('Renewable vs NonRenewable Source Output Colored by Energy Price')\nplt.xlabel('Renewable Source Output')\nplt.ylabel('NonRenewable Source Output')\n\nnorm = plt.Normalize(df['Energy_Price'].min(), df['Energy_Price'].max())\nsm = plt.cm.ScalarMappable(cmap='viridis', norm=norm)\nsm.set_array([])\ncbar = plt.colorbar(sm, label='Energy Price')\nsave_plot(\"scatter_renewable_vs_nonrenewable\")\n\n# -------------------------------\n# 9. Zip the Directory for Download\n# -------------------------------\nzip_path = \"/kaggle/working/Dataset_plots.zip\"\nwith zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n    for root, _, files in os.walk(plot_dir):\n        for file in files:\n            zipf.write(os.path.join(root, file), arcname=file)\n\n# -------------------------------\n# 10. Provide Download Link\n# -------------------------------\nprint(f\"All plots have been saved in {plot_dir} and zipped as {zip_path}. You can download it from the Kaggle working directory.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport zipfile\nfrom matplotlib import pyplot as plt\n\n# Create a directory to save plots\noutput_dir = 'plots'\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n\n# Function to save and close the plot\ndef save_plot(filename):\n    plt.savefig(os.path.join(output_dir, filename), bbox_inches='tight')\n    plt.close()\n\n# -------------------------------\n# 2. Time Series Analysis\n# -------------------------------\nplt.figure(figsize=(14, 6))\nplt.plot(df['Timestamp'], df['Energy_Demand'], label='Energy Demand', color='blue')\nplt.plot(df['Timestamp'], df['Energy_Supply'], label='Energy Supply', color='green')\nplt.plot(df['Timestamp'], df['Temperature'], label='Temperature', color='red')\nplt.xlabel('Timestamp')\nplt.ylabel('Value')\nplt.title('Time Series of Energy Demand, Energy Supply, and Temperature')\nplt.legend()\nplt.xticks(rotation=45)\nplt.tight_layout()\nsave_plot('time_series_energy_temp.png')\n\n# -------------------------------\n# 3. Distribution Analysis: Histograms & KDE Plots\n# -------------------------------\n# Histograms for numerical features\ndf[numerical_cols].hist(bins=15, figsize=(15, 10), layout=(3, 3))\nplt.suptitle(\"Histograms of Numerical Features\", fontsize=16)\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])\nsave_plot('histograms_numerical_features.png')\n\n# KDE plots for numerical features\nplt.figure(figsize=(14, 8))\nfor col in numerical_cols:\n    sns.kdeplot(data=df, x=col, fill=True, label=col)\nplt.title('KDE Plot of Numerical Features')\nplt.xlabel('Value')\nplt.legend()\nsave_plot('kde_numerical_features.png')\n\n# -------------------------------\n# 4. Correlation Analysis: Heatmap and Pairplot\n# -------------------------------\n# Correlation Heatmap\ncorr = df[numerical_cols].corr()\nplt.figure(figsize=(10, 8))\nsns.heatmap(corr, annot=True, cmap='coolwarm', fmt=\".2f\")\nplt.title(\"Correlation Heatmap of Numerical Features\")\nsave_plot('correlation_heatmap.png')\n\n# Pairplot for pairwise relationships\nsns.pairplot(df[numerical_cols])\nplt.suptitle(\"Pairwise Relationships Between Numerical Features\", y=1.02)\nsave_plot('pairplot_numerical_features.png')\n\n# -------------------------------\n# 5. Categorical Analysis: Weather Conditions\n# -------------------------------\n# Count plot for Weather_Condition_x\nplt.figure(figsize=(8, 4))\nsns.countplot(x='Weather_Condition_x', data=df, palette='Set2')\nplt.title('Count of Observations by Weather_Condition_x')\nplt.xlabel('Weather Condition (x)')\nplt.ylabel('Count')\nsave_plot('countplot_weather_condition_x.png')\n\n# Count plot for Weather_Condition_y\nplt.figure(figsize=(8, 4))\nsns.countplot(x='Weather_Condition_y', data=df, palette='Set3')\nplt.title('Count of Observations by Weather_Condition_y')\nplt.xlabel('Weather Condition (y)')\nplt.ylabel('Count')\nsave_plot('countplot_weather_condition_y.png')\n\n# Box plots for energy demand and supply by Weather_Condition_x\nplt.figure(figsize=(14, 6))\nplt.subplot(1, 2, 1)\nsns.boxplot(x='Weather_Condition_x', y='Energy_Demand', data=df, palette='pastel')\nplt.title('Energy Demand by Weather_Condition_x')\nplt.xticks(rotation=45)\n\nplt.subplot(1, 2, 2)\nsns.boxplot(x='Weather_Condition_x', y='Energy_Supply', data=df, palette='pastel')\nplt.title('Energy Supply by Weather_Condition_x')\nplt.xticks(rotation=45)\n\nplt.tight_layout()\nsave_plot('boxplot_weather_condition_x.png')\n\n# -------------------------------\n# 6. Scatter Plots: Exploring Relationships\n# -------------------------------\n# Scatter plot: Grid Load vs. Energy Demand\nplt.figure(figsize=(10, 6))\nsns.scatterplot(x='Grid_Load', y='Energy_Demand', data=df, hue='Weather_Condition_x', palette='deep')\nplt.title('Grid Load vs. Energy Demand')\nplt.xlabel('Grid Load')\nplt.ylabel('Energy Demand')\nplt.legend(title='Weather Condition')\nsave_plot('scatter_grid_load_vs_energy_demand.png')\n\n# Scatter plot: Renewable vs NonRenewable Source Output\nplt.figure(figsize=(10, 6))\nscatter = sns.scatterplot(\n    x='Renewable_Source_Output',\n    y='NonRenewable_Source_Output',\n    data=df,\n    hue='Energy_Price',\n    palette='viridis',\n    legend=False\n)\nplt.title('Renewable vs NonRenewable Source Output Colored by Energy Price')\nplt.xlabel('Renewable Source Output')\nplt.ylabel('NonRenewable Source Output')\n\n# Colorbar\nnorm = plt.Normalize(df['Energy_Price'].min(), df['Energy_Price'].max())\nsm = plt.cm.ScalarMappable(cmap='viridis', norm=norm)\nsm.set_array([])\nplt.colorbar(sm, label='Energy Price')\nsave_plot('scatter_renewable_vs_nonrenewable.png')\n\n# -------------------------------\n# 7. Combined Time Series Plot with Dual Y-axis\n# -------------------------------\nfig, ax1 = plt.subplots(figsize=(14, 6))\n\nax1.plot(df['Timestamp'], df['Energy_Demand'], label='Energy Demand', color='blue')\nax1.plot(df['Timestamp'], df['Energy_Supply'], label='Energy Supply', color='green')\nax1.plot(df['Timestamp'], df['Temperature'], label='Temperature', color='red')\nax1.set_xlabel('Timestamp')\nax1.set_ylabel('Energy/Temperature Values')\nax1.tick_params(axis='x', rotation=45)\nax1.legend(loc='upper left')\n\n# Secondary y-axis for Energy Price\nax2 = ax1.twinx()\nax2.plot(df['Timestamp'], df['Energy_Price'], label='Energy Price', color='purple', linestyle='--')\nax2.set_ylabel('Energy Price')\nax2.legend(loc='upper right')\n\nplt.title(\"Combined Time Series Plot: Energy Metrics and Price\")\nplt.tight_layout()\nsave_plot('combined_time_series_dual_axis.png')\n\n# -------------------------------\n# Zipping the Plots\n# -------------------------------\nzip_filename = 'Dataset_files.zip'\nwith zipfile.ZipFile(zip_filename, 'w') as zipf:\n    for root, dirs, files in os.walk(output_dir):\n        for file in files:\n            zipf.write(os.path.join(root, file), arcname=file)\n\nprint(f'All plots have been saved and zipped as {zip_filename}.')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Specify the path to your dataset\nfile_path = \"/kaggle/input/integrated-energy-management-and-forecasting/Integrated Energy Management and Forecasting Dataset.csv\"\n\n# Read the CSV file into a DataFrame\ndf = pd.read_csv(file_path)\n\n# Print information about the DataFrame (such as column names, non-null counts, and data types)\nprint(\"Data Information:\")\nprint(df.info())\n\n# Print the first 5 rows of the DataFrame\nprint(\"\\nFirst 5 Rows of Data:\")\nprint(df.head())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check unique values in both columns\nprint(df[['Weather_Condition_x', 'Weather_Condition_y']].drop_duplicates())\n\n# Compare if they are the same\nprint((df['Weather_Condition_x'] == df['Weather_Condition_y']).value_counts())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Specify the path to your dataset\nfile_path = \"/kaggle/input/integrated-energy-management-and-forecasting/Integrated Energy Management and Forecasting Dataset.csv\"\n\n# Read the CSV file into a DataFrame\ndf = pd.read_csv(file_path)\n\n# Print basic DataFrame information\nprint(\"Data Information:\")\ndf.info()  # This prints the info to stdout\n\n# Print the first 5 rows of the DataFrame\nprint(\"\\nFirst 5 Rows of Data:\")\nprint(df.head())\n\n# Convert the Timestamp column to datetime format for easier time-based operations\ndf['Timestamp'] = pd.to_datetime(df['Timestamp'])\nprint(\"\\nData Types After Converting Timestamp:\")\nprint(df.dtypes)\n\n# Print summary statistics for numeric columns\nprint(\"\\nSummary Statistics:\")\nprint(df.describe())\n\n# If you want to inspect the weather condition columns, you can print unique values\nprint(\"\\nUnique values in 'Weather_Condition_x':\")\nprint(df['Weather_Condition_x'].unique())\n\nprint(\"\\nUnique values in 'Weather_Condition_y':\")\nprint(df['Weather_Condition_y'].unique())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Plot Energy Demand over Time\nplt.figure(figsize=(10, 6))\nplt.plot(df['Timestamp'], df['Energy_Demand'], label='Energy Demand')\nplt.xlabel('Time')\nplt.ylabel('Energy Demand')\nplt.title('Energy Demand Over Time')\nplt.legend()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"import pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport random\nimport numpy as np\n\n# -------------------------------\n# 1. Load and Preprocess the Dataset\n# -------------------------------\n# Specify the path to your dataset (update the path as needed)\nfile_path = \"/kaggle/input/integrated-energy-management-and-forecasting/Integrated Energy Management and Forecasting Dataset.csv\"\n\n# Read the CSV file into a DataFrame\ndf = pd.read_csv(file_path)\n\n# Optionally, sort the data by timestamp if needed\ndf['Timestamp'] = pd.to_datetime(df['Timestamp'])\ndf = df.sort_values(\"Timestamp\")\n\n# Use the \"Energy_Demand\" column as the time series for forecasting\nenergy_series = df[\"Energy_Demand\"].values.astype(np.float32)\n\n# (Optional) Normalize or scale the data\n# For example, here we simply convert it to a torch tensor.\ntime_series_data = torch.tensor(energy_series)  # Shape: [n_points]\n\nprint(\"Loaded time series of length:\", len(time_series_data))\n\n\n# -------------------------------\n# 2. Define the Base Time Series Model (LSTM)\n# -------------------------------\nclass TimeSeriesLSTM(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, output_size):\n        super(TimeSeriesLSTM, self).__init__()\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, output_size)\n        \n        # Save parameters for cloning later\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.output_size = output_size\n        \n    def forward(self, x):\n        # x shape: (batch_size, seq_len, input_size)\n        out, _ = self.lstm(x)\n        # Use the output from the last time step\n        out = out[:, -1, :]\n        out = self.fc(out)\n        return out\n\n# -------------------------------\n# 3. Define the MAML Framework\n# -------------------------------\nclass MAML:\n    def __init__(self, model, lr_inner=0.01, lr_meta=0.001, inner_steps=5):\n        self.model = model\n        self.lr_inner = lr_inner\n        self.lr_meta = lr_meta\n        self.inner_steps = inner_steps\n        \n        # Meta-optimizer to update the model's initialization\n        self.meta_optimizer = optim.Adam(self.model.parameters(), lr=self.lr_meta)\n    \n    def inner_loop(self, support_x, support_y):\n        \n        Create a temporary copy of the model and perform a few gradient steps on the support set.\n        \n        # Create a new model instance and load current parameters\n        fast_model = TimeSeriesLSTM(self.model.input_size,\n                                    self.model.hidden_size,\n                                    self.model.num_layers,\n                                    self.model.output_size)\n        fast_model.load_state_dict(self.model.state_dict())\n        fast_model.train()\n        \n        inner_optimizer = optim.SGD(fast_model.parameters(), lr=self.lr_inner)\n        criterion = nn.MSELoss()\n        \n        for _ in range(self.inner_steps):\n            inner_optimizer.zero_grad()\n            preds = fast_model(support_x)\n            loss = criterion(preds, support_y)\n            loss.backward()\n            inner_optimizer.step()\n            \n        return fast_model\n\n    def meta_update(self, tasks):\n        \n        Perform a meta-update over a batch of tasks.\n        Each tak consists of a support and query set.\n        \n        self.meta_optimizer.zero_grad()\n        criterion = nn.MSELoss()\n        meta_loss = 0.0\n        \n        for support_x, support_y, query_x, query_y in tasks:\n            # Obtain an adapted model for the current task\n            fast_model = self.inner_loop(support_x, support_y)\n            \n            # Evaluate the adapted model on the query set\n            fast_model.eval()\n            preds = fast_model(query_x)\n            loss = criterion(preds, query_y)\n            meta_loss += loss\n        \n        # Average the meta loss across tasks\n        meta_loss = meta_loss / len(tasks)\n        meta_loss.backward()\n        self.meta_optimizer.step()\n        return meta_loss.item()\n\n# -------------------------------\n# 4. Define a Task Generator from the Dataset\n# -------------------------------\ndef generate_time_series_task_from_data(seq_len=20, task_size=60):\n    \n    Generate a forecasting task from the global time_series_data.\n    \n    Args:\n        seq_len (int): The length of the input sequence window.\n        task_size (int): The total length of the task segment to sample.\n                         Must be greater than seq_len.\n                         \n    Returns:\n        support_x, support_y, query_x, query_y: Tensors for support and query sets.\n    \n    global time_series_data\n    \n    # Ensure there is enough data to sample a full task\n    total_points = len(time_series_data)\n    if total_points < task_size:\n        raise ValueError(\"Not enough data points to sample the task.\")\n        \n    # Randomly choose a starting index\n    max_start = total_points - task_size\n    start_idx = random.randint(0, max_start)\n    task_segment = time_series_data[start_idx:start_idx + task_size]\n    \n    # Create sliding windows: each input is a window of length seq_len and target is the next value\n    inputs = []\n    targets = []\n    for i in range(len(task_segment) - seq_len):\n        window = task_segment[i:i+seq_len].unsqueeze(1)  # shape: (seq_len, 1)\n        target = task_segment[i+seq_len].unsqueeze(0)      # shape: (1,)\n        inputs.append(window)\n        targets.append(target)\n    \n    inputs = torch.stack(inputs)   # shape: (num_windows, seq_len, 1)\n    targets = torch.stack(targets) # shape: (num_windows, 1)\n    \n    # Split into support and query sets (e.g., first half support, second half query)\n    split = inputs.shape[0] // 2\n    support_x = inputs[:split]\n    support_y = targets[:split]\n    query_x = inputs[split:]\n    query_y = targets[split:]\n    \n    return support_x, support_y, query_x, query_y\n\n# -------------------------------\n# 5. Set Up the Model and MAML Training\n# -------------------------------\n# Hyperparameters for the LSTM model\ninput_size = 1      # one feature (Energy_Demand)\nhidden_size = 32\nnum_layers = 1\noutput_size = 1\n\n# Create the base model instance\nbase_model = TimeSeriesLSTM(input_size, hidden_size, num_layers, output_size)\n\n# Create the MAML meta-learner instance\nmaml = MAML(base_model, lr_inner=0.01, lr_meta=0.001, inner_steps=5)\n\n# -------------------------------\n# 6. Meta-Training Loop\n# -------------------------------\nnum_meta_iterations = 1000   # Total meta-training iterations\ntasks_per_meta_update = 4    # Number of tasks per meta-update\n\nfor iteration in range(num_meta_iterations):\n    tasks = []\n    for _ in range(tasks_per_meta_update):\n        support_x, support_y, query_x, query_y = generate_time_series_task_from_data(seq_len=20, task_size=60)\n        tasks.append((support_x, support_y, query_x, query_y))\n        \n    meta_loss = maml.meta_update(tasks)\n    \n    if iteration % 100 == 0:\n        print(f\"Iteration {iteration}, Meta Loss: {meta_loss:.4f}\")\n\n# -------------------------------\n# 7. Evaluation on a New Task from the Dataset\n# -------------------------------\n# Generate a new task from the dataset\nsupport_x, support_y, query_x, query_y = generate_time_series_task_from_data(seq_len=20, task_size=60)\n\n# Fine-tune the base model on the support set of the new task\nadapted_model = maml.inner_loop(support_x, support_y)\n\nadapted_model.eval()\nwith torch.no_grad():\n    predictions = adapted_model(query_x)\n    test_loss = nn.MSELoss()(predictions, query_y)\n    \nprint(f\"Test Loss on the new task: {test_loss.item():.4f}\")\n\"\"\"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"import pandas as pd\n\n# Specify the path to your dataset\nfile_path = \"/kaggle/input/predicting-electricity-consumption/train.csv\"\n\n# Read the CSV file into a DataFrame\ndf = pd.read_csv(file_path)\n\n# Print basic DataFrame information\nprint(\"Data Information:\")\ndf.info()  # This prints the info to stdout\n\n# Print the first 5 rows of the DataFrame\nprint(\"\\nFirst 5 Rows of Data:\")\nprint(df.head())\n\"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"import pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport random\nimport numpy as np\n\n# Set random seeds for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\nrandom.seed(42)\n\n# Use GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n# -----------------------------------\n# 1. Load and Preprocess the Dataset\n# -----------------------------------\nfile_path = \"/kaggle/input/predicting-electricity-consumption/train.csv\"\ndf = pd.read_csv(file_path)\n\n# Convert timestamp to datetime and sort by time\ndf['timestamp'] = pd.to_datetime(df['timestamp'])\ndf = df.sort_values(\"timestamp\")\n\n# Drop rows with missing meter_reading values\ndf = df.dropna(subset=[\"meter_reading\"])\n\nprint(\"Data Information:\")\ndf.info()\n\nprint(\"\\nFirst 5 Rows of Data:\")\nprint(df.head())\n\n# Group the data by building_id (each building is its own time series)\nbuilding_groups = dict(tuple(df.groupby(\"building_id\")))\nprint(\"\\nTotal buildings in dataset:\", len(building_groups))\n\n# -----------------------------------\n# 2. Data Sample Selection: Choose Training and Testing Buildings\n# -----------------------------------\n# Set the number of buildings you want to use for training and testing.\ntrain_building_count = 100  # Adjust as desired\ntest_building_count = 50   # Adjust as desired\n\nall_buildings = list(building_groups.keys())\n# Randomly sample training buildings (without replacement)\ntrain_buildings = random.sample(all_buildings, min(train_building_count, len(all_buildings)))\n# For testing, use buildings not in the training set.\nremaining_buildings = list(set(all_buildings) - set(train_buildings))\ntest_buildings = random.sample(remaining_buildings, min(test_building_count, len(remaining_buildings)))\n\nprint(\"Number of training buildings:\", len(train_buildings))\nprint(\"Number of test buildings:\", len(test_buildings))\n\n# Compute global statistics for meter_reading for normalization\nmeter_mean = df[\"meter_reading\"].mean()\nmeter_std = df[\"meter_reading\"].std()\n\n# -----------------------------------\n# 3. Define the Enhanced Base Time Series Model (LSTM)\n# -----------------------------------\nclass EnhancedTimeSeriesLSTM(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.3):\n        super(EnhancedTimeSeriesLSTM, self).__init__()\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, \n                            batch_first=True, dropout=dropout)\n        # Batch normalization applied to the LSTM output features\n        self.batch_norm = nn.BatchNorm1d(hidden_size)\n        # Fully connected head\n        self.fc = nn.Linear(hidden_size, output_size)\n        \n        # Save architecture details for cloning later\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.output_size = output_size\n        \n    def forward(self, x):\n        # x: (batch_size, seq_len, input_size)\n        lstm_out, _ = self.lstm(x)  \n        # Use the output of the last time step\n        last_out = lstm_out[:, -1, :]  # shape: (batch_size, hidden_size)\n        # Batch normalization expects a 2D tensor\n        normed = self.batch_norm(last_out)\n        out = self.fc(normed)\n        return out\n\n# -----------------------------------\n# 4. Define the MAML Framework with Gradient Clipping\n# -----------------------------------\nclass MAML:\n    def __init__(self, model, lr_inner=0.01, lr_meta=0.001, inner_steps=5, clip_grad=1.0):\n        self.model = model.to(device)\n        self.lr_inner = lr_inner\n        self.lr_meta = lr_meta\n        self.inner_steps = inner_steps\n        self.clip_grad = clip_grad\n        \n        # Meta-optimizer for the model's initialization parameters\n        self.meta_optimizer = optim.Adam(self.model.parameters(), lr=self.lr_meta)\n    \n    def inner_loop(self, support_x, support_y):\n        \n        Create a temporary copy of the model and perform a few gradient steps on the support set.\n        \n        # Clone the model architecture and load current weights\n        fast_model = EnhancedTimeSeriesLSTM(self.model.input_size,\n                                            self.model.hidden_size,\n                                            self.model.num_layers,\n                                            self.model.output_size).to(device)\n        fast_model.load_state_dict(self.model.state_dict())\n        fast_model.train()\n        \n        inner_optimizer = optim.SGD(fast_model.parameters(), lr=self.lr_inner)\n        criterion = nn.MSELoss()\n        \n        for _ in range(self.inner_steps):\n            inner_optimizer.zero_grad()\n            preds = fast_model(support_x)\n            loss = criterion(preds, support_y)\n            loss.backward()\n            # Clip gradients to avoid explosion\n            torch.nn.utils.clip_grad_norm_(fast_model.parameters(), self.clip_grad)\n            inner_optimizer.step()\n            \n        return fast_model\n\n    def meta_update(self, tasks):\n        \n        Perform a meta-update over a batch of tasks.\n        \n        self.meta_optimizer.zero_grad()\n        criterion = nn.MSELoss()\n        meta_loss = 0.0\n        \n        for support_x, support_y, query_x, query_y in tasks:\n            # Ensure tensors are on the device\n            support_x, support_y = support_x.to(device), support_y.to(device)\n            query_x, query_y = query_x.to(device), query_y.to(device)\n            \n            fast_model = self.inner_loop(support_x, support_y)\n            # Do not call fast_model.eval() here! Keep it in training mode for backward compatibility.\n            preds = fast_model(query_x)\n            loss = criterion(preds, query_y)\n            meta_loss += loss\n        \n        meta_loss = meta_loss / len(tasks)\n        meta_loss.backward()\n        self.meta_optimizer.step()\n        return meta_loss.item()\n\n# -----------------------------------\n# 5. Define an Enhanced Task Generator Function\n# -----------------------------------\ndef generate_time_series_task(seq_len=20, task_size=60, allowed_buildings=None):\n    \n    Generate a forecasting task by sampling a contiguous segment from a randomly selected building.\n    The meter readings are normalized using global mean and std.\n    \n    Args:\n        seq_len (int): Length of the input window.\n        task_size (int): Total records in the task segment (must be > seq_len).\n        allowed_buildings (list or None): List of building IDs to sample from. If None, all buildings are allowed.\n    \n    Returns:\n        support_x, support_y, query_x, query_y: Tensors for the support and query sets.\n    \n    # Determine valid buildings based on allowed_buildings and sufficient data points\n    if allowed_buildings is not None:\n        valid_buildings = [bid for bid in allowed_buildings if len(building_groups[bid]) >= task_size]\n    else:\n        valid_buildings = [bid for bid, group in building_groups.items() if len(group) >= task_size]\n    \n    if not valid_buildings:\n        raise ValueError(\"No building has enough data for the selected task_size.\")\n    \n    building_id = random.choice(valid_buildings)\n    group = building_groups[building_id].sort_values(\"timestamp\")\n    \n    # Extract meter_readings and apply normalization\n    meter_readings = group[\"meter_reading\"].values.astype(np.float32)\n    meter_readings = (meter_readings - meter_mean) / meter_std  # normalize globally\n    \n    total_points = len(meter_readings)\n    max_start = total_points - task_size\n    start_idx = random.randint(0, max_start)\n    task_segment = meter_readings[start_idx:start_idx + task_size]\n    \n    # Convert to torch tensor\n    task_segment = torch.tensor(task_segment, dtype=torch.float32)\n    \n    # Build sliding windows: input = window of seq_len, target = next value\n    inputs = []\n    targets = []\n    for i in range(len(task_segment) - seq_len):\n        window = task_segment[i:i+seq_len].unsqueeze(1)  # shape: (seq_len, 1)\n        target = task_segment[i+seq_len].unsqueeze(0)      # shape: (1,)\n        inputs.append(window)\n        targets.append(target)\n    \n    inputs = torch.stack(inputs)   # shape: (num_windows, seq_len, 1)\n    targets = torch.stack(targets) # shape: (num_windows, 1)\n    \n    # Split into support and query sets (50/50 split)\n    split = inputs.shape[0] // 2\n    support_x = inputs[:split]\n    support_y = targets[:split]\n    query_x = inputs[split:]\n    query_y = targets[split:]\n    \n    return support_x, support_y, query_x, query_y\n\n# -----------------------------------\n# 6. Set Up the Enhanced Model and Meta-Learner\n# -----------------------------------\n# Hyperparameters for the enhanced LSTM model\ninput_size = 1       # one feature: normalized meter_reading\nhidden_size = 64     # increased hidden size for better capacity\nnum_layers = 2       # two LSTM layers\noutput_size = 1\ndropout = 0.3        # dropout for regularization\n\nbase_model = EnhancedTimeSeriesLSTM(input_size, hidden_size, num_layers, output_size, dropout)\nmaml = MAML(base_model, lr_inner=0.01, lr_meta=0.001, inner_steps=5, clip_grad=1.0)\n\n# -----------------------------------\n# 7. Enhanced Meta-Training Loop\n# -----------------------------------\nnum_meta_iterations = 1000   # Total meta-training iterations\ntasks_per_meta_update = 4    # Number of tasks per meta-update\n\nfor iteration in range(num_meta_iterations):\n    tasks = []\n    for _ in range(tasks_per_meta_update):\n        # Use only the training buildings for meta-training\n        support_x, support_y, query_x, query_y = generate_time_series_task(seq_len=20, task_size=60, allowed_buildings=train_buildings)\n        tasks.append((support_x, support_y, query_x, query_y))\n        \n    meta_loss = maml.meta_update(tasks)\n    \n    if iteration % 100 == 0:\n        print(f\"Iteration {iteration}, Meta Loss: {meta_loss:.4f}\")\n\n# -----------------------------------\n# 8. Evaluation on New Tasks from the Test Buildings\n# -----------------------------------\n# Generate a set of test tasks (e.g., 5 tasks) and report the average test loss.\ntest_tasks = []\nnum_test_tasks = 5\nfor _ in range(num_test_tasks):\n    support_x, support_y, query_x, query_y = generate_time_series_task(seq_len=20, task_size=60, allowed_buildings=test_buildings)\n    # Move evaluation data to the device\n    support_x, support_y = support_x.to(device), support_y.to(device)\n    query_x, query_y = query_x.to(device), query_y.to(device)\n    test_tasks.append((support_x, support_y, query_x, query_y))\n\ntest_loss_total = 0.0\nfor support_x, support_y, query_x, query_y in test_tasks:\n    adapted_model = maml.inner_loop(support_x, support_y)\n    adapted_model.eval()  # For pure prediction here, it's okay to set eval mode after adaptation.\n    with torch.no_grad():\n        preds = adapted_model(query_x)\n        loss = nn.MSELoss()(preds, query_y)\n        test_loss_total += loss.item()\n\navg_test_loss = test_loss_total / num_test_tasks\nprint(f\"Average Test Loss on {num_test_tasks} test tasks: {avg_test_loss:.4f}\")\n\"\"\"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 1.  TCN LSTM GRUs Incremental Learning Architecture","metadata":{}},{"cell_type":"code","source":"\"\"\"import pandas as pd\nimport tensorflow as tf\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\nfrom collections import deque\nimport random\nfrom tensorflow.keras.layers import Conv1D, BatchNormalization, ReLU, Add, Dropout\nimport time\n\n# ===========================\n# 1. Data Loading and Preprocessing\n# ===========================\ndata = pd.read_csv('/kaggle/input/integrated-energy-management-and-forecasting/Integrated Energy Management and Forecasting Dataset.csv')\n\n# Convert Timestamp to datetime and sort\ndata['Timestamp'] = pd.to_datetime(data['Timestamp'])\ndata.sort_values('Timestamp', inplace=True)\n\n# Select the 'Energy_Demand' feature and normalize it\nfeatures = ['Energy_Demand']\nscaler = MinMaxScaler()\ndata_features = scaler.fit_transform(data[features].values)\n\nsequence_length = 48\nprediction_length = 48\n\n# Create sequences and labels using a sliding window approach\nsequences, labels = [], []\nfor i in range(len(data_features) - sequence_length - prediction_length + 1):\n    seq = data_features[i:i + sequence_length]\n    label = data_features[i + sequence_length:i + sequence_length + prediction_length]\n    sequences.append(seq)\n    labels.append(label)\n\nsequences = np.array(sequences)\nlabels = np.array(labels).astype(np.float32)\n\n# ===========================\n# 2. Replay Buffer for Incremental Learning\n# ===========================\nclass ReplayBuffer:\n    def __init__(self, max_size=1000):\n        self.buffer = deque(maxlen=max_size)\n\n    def add(self, sequence, label):\n        self.buffer.append((sequence, label))\n\n    def sample(self, batch_size):\n        # If the replay buffer is not yet full, return all samples\n        if len(self.buffer) < batch_size:\n            return list(self.buffer)\n        else:\n            return random.sample(self.buffer, batch_size)\n\n# ===========================\n# 3. Define TCN Block and Build the Hybrid Model (TCN → GRU → LSTM)\n# ===========================\ndef tcn_block(filters, kernel_size, dilation_rate, dropout):\n    def block(x):\n        # Residual connection via 1x1 convolution to match dimensions\n        res = Conv1D(filters, kernel_size=1, padding=\"same\")(x)\n        # First convolution branch with causal padding\n        conv1 = Conv1D(filters, kernel_size, dilation_rate=dilation_rate, padding=\"causal\", activation=None)(x)\n        norm1 = BatchNormalization()(conv1)\n        act1 = ReLU()(norm1)\n        drop1 = Dropout(dropout)(act1)\n        # Second convolution branch with causal padding\n        conv2 = Conv1D(filters, kernel_size, dilation_rate=dilation_rate, padding=\"causal\", activation=None)(drop1)\n        norm2 = BatchNormalization()(conv2)\n        act2 = ReLU()(norm2)\n        drop2 = Dropout(dropout)(act2)\n        # Skip connection: add the residual and the processed path\n        skip = Add()([res, drop2])\n        return skip\n    return block\n\ndef build_hybrid_model(seq_len=sequence_length, pred_len=prediction_length, dropout=0.4, hidden_dim=128):\n    inputs = tf.keras.layers.Input(shape=(seq_len, len(features)))\n\n    # Three TCN blocks with increasing dilation rates\n    tcn_output = tcn_block(hidden_dim, kernel_size=3, dilation_rate=1, dropout=dropout)(inputs)\n    #tcn_output = tcn_block(hidden_dim, kernel_size=3, dilation_rate=2, dropout=dropout)(tcn_output)\n    #tcn_output = tcn_block(hidden_dim, kernel_size=3, dilation_rate=4, dropout=dropout)(tcn_output)\n\n    # Bidirectional GRU layer\n    gru_output = tf.keras.layers.Bidirectional(\n        tf.keras.layers.GRU(hidden_dim, dropout=dropout, return_sequences=True, kernel_initializer='orthogonal')\n    )(tcn_output)\n\n    # Bidirectional LSTM layer\n    lstm_output = tf.keras.layers.Bidirectional(\n        tf.keras.layers.LSTM(hidden_dim, dropout=dropout, return_sequences=True, kernel_initializer='orthogonal')\n    )(gru_output)\n\n    # Use the first 'prediction_length' time steps of the LSTM output for prediction\n    truncated = lstm_output[:, :pred_len, :]\n    outputs = tf.keras.layers.Dense(1, activation='linear')(truncated)\n\n    # Compile the model with Adam optimizer and mean squared error loss\n    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n    model.compile(\n        optimizer=tf.optimizers.Adam(learning_rate=0.001),\n        loss='mse',\n        metrics=['mae', 'mse', tf.keras.metrics.MeanAbsolutePercentageError(name='mape')]\n    )\n    return model\n\n# Build and visualize the model architecture\nhybrid_model_tcn_gru_lstm = build_hybrid_model()\nhybrid_model_tcn_gru_lstm.summary()\n\ntf.keras.utils.plot_model(\n    hybrid_model_tcn_gru_lstm,\n    to_file=\"hybrid_model_tcn_gru_lstm_architecture.png\",\n    show_shapes=True,\n    show_layer_names=True\n)\n\n# ===========================\n# 4. Incremental Online Training Loop with Per-Epoch Metrics\n# ===========================\n# Training configuration\nchunk_batch_size = 20         # Number of new samples per update (online chunk)\nepochs_per_chunk =  150         # Epochs per update\nreplay_sample_size = 32       # Number of samples to draw from replay buffer per update\n\n# Dictionaries to store per-epoch metrics\nmetrics_history = {\n    \"epoch\": [],\n    \"loss\": [],\n    \"val_loss\": [],\n    \"accuracy\": [],\n    \"val_accuracy\": []\n}\n# Also record training time per epoch\nepoch_times = []\n\nreplay_buffer = ReplayBuffer(max_size=5000)\nglobal_epoch = 0\n\n# Process the sequences in an online manner (chunk by chunk)\nnum_chunks = int(np.ceil(len(sequences) / chunk_batch_size))\nprint(f\"Starting online incremental training over {num_chunks} chunks...\")\n\nfor chunk_idx in range(0, len(sequences), chunk_batch_size):\n    # Get new incoming data for this chunk\n    new_sequences = sequences[chunk_idx:chunk_idx + chunk_batch_size]\n    new_labels = labels[chunk_idx:chunk_idx + chunk_batch_size]\n    \n    # Add new samples to the replay buffer\n    for seq, lbl in zip(new_sequences, new_labels):\n        replay_buffer.add(seq, lbl)\n    \n    # Sample replay data for this update\n    replay_samples = replay_buffer.sample(replay_sample_size)\n    replay_sequences, replay_labels = zip(*replay_samples)\n    replay_sequences = np.array(replay_sequences)\n    replay_labels = np.array(replay_labels)\n    \n    # Combine the new data with replay data to form a training mini-batch\n    train_sequences = np.vstack((new_sequences, replay_sequences))\n    train_labels = np.vstack((new_labels, replay_labels))\n    \n    # Train for a fixed number of epochs on this combined data (simulate online updates)\n    start_time_chunk = time.time()\n    history = hybrid_model_tcn_gru_lstm.fit(\n        train_sequences, train_labels,\n        batch_size=chunk_batch_size,\n        epochs=epochs_per_chunk,\n        verbose=0,\n        validation_split=0.1\n    )\n    end_time_chunk = time.time()\n    \n    # Record per-epoch metrics for this chunk update\n    chunk_time = end_time_chunk - start_time_chunk\n    avg_epoch_time = chunk_time / epochs_per_chunk\n    for epoch in range(epochs_per_chunk):\n        global_epoch += 1\n        loss = history.history['loss'][epoch]\n        val_loss = history.history['val_loss'][epoch]\n        # Define \"accuracy\" as 1 - loss (for visualization purposes)\n        acc = 1 - loss\n        val_acc = 1 - val_loss\n        metrics_history[\"epoch\"].append(global_epoch)\n        metrics_history[\"loss\"].append(loss)\n        metrics_history[\"val_loss\"].append(val_loss)\n        metrics_history[\"accuracy\"].append(acc)\n        metrics_history[\"val_accuracy\"].append(val_acc)\n        epoch_times.append(avg_epoch_time)\n    \n    print(f\"Chunk {chunk_idx//chunk_batch_size + 1}/{num_chunks}: \"\n          f\"Avg Loss={np.mean(history.history['loss']):.4f}, \"\n          f\"Avg Val Loss={np.mean(history.history['val_loss']):.4f}, \"\n          f\"Time for chunk={chunk_time:.2f}s\")\n\n# Save the trained model\nmodel_save_path = \"hybrid_model_tcn_gru_lstm.h5\"\nhybrid_model_tcn_gru_lstm.save(model_save_path)\nprint(f\"Model saved to {model_save_path}\")\n\n# ===========================\n# 5. Visualization of Performance Metrics per Epoch\n# ===========================\n# Plot Training and Validation Loss per Epoch\nplt.figure(figsize=(10, 6))\nplt.plot(metrics_history[\"epoch\"], metrics_history[\"loss\"], label=\"Training Loss\", marker=\"o\")\nplt.plot(metrics_history[\"epoch\"], metrics_history[\"val_loss\"], label=\"Validation Loss\", marker=\"s\", linestyle=\"--\")\nplt.title(\"Training and Validation Loss per Epoch\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.grid(True, linestyle=\"--\", alpha=0.6)\nplt.tight_layout()\nplt.savefig(\"hybrid_loss_per_epoch.png\")\nplt.savefig(\"hybrid_loss_per_epoch.pdf\")\nplt.savefig(\"hybrid_loss_per_epoch.eps\")\nplt.show()\n\n# Plot Training and Validation Accuracy per Epoch\nplt.figure(figsize=(10, 6))\nplt.plot(metrics_history[\"epoch\"], metrics_history[\"accuracy\"], label=\"Training Accuracy\", marker=\"o\")\nplt.plot(metrics_history[\"epoch\"], metrics_history[\"val_accuracy\"], label=\"Validation Accuracy\", marker=\"s\", linestyle=\"--\")\nplt.title(\"Training and Validation Accuracy per Epoch\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.grid(True, linestyle=\"--\", alpha=0.6)\nplt.tight_layout()\nplt.savefig(\"hybrid_accuracy_per_epoch.png\")\nplt.savefig(\"hybrid_accuracy_per_epoch.pdf\")\nplt.savefig(\"hybrid_accuracy_per_epoch.eps\")\nplt.show()\n\n# Plot Training Time per Epoch\nplt.figure(figsize=(10, 6))\nplt.bar(metrics_history[\"epoch\"], epoch_times, color=\"blue\", alpha=0.7)\nplt.title(\"Training Time per Epoch\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Time (seconds)\")\nplt.grid(True, linestyle=\"--\", alpha=0.6)\nplt.tight_layout()\nplt.savefig(\"hybrid_time_per_epoch.png\")\nplt.savefig(\"hybrid_time_per_epoch.pdf\")\nplt.savefig(\"hybrid_time_per_epoch.eps\")\nplt.show()\n\n# ===========================\n# 6. Forecasting Visualization\n# ===========================\n# Generate forecast using the last available sequence\npredicted_values = hybrid_model_tcn_gru_lstm.predict(np.expand_dims(sequences[-1], axis=0)).flatten()\nactual_values = labels[-1].flatten()\n\nplt.figure(figsize=(12, 8))\nplt.plot(range(len(actual_values)), actual_values, label=\"Actual Energy Consumption\", color=\"black\", marker=\"o\")\nplt.plot(range(len(predicted_values)), predicted_values, label=\"Hybrid Model Forecast\", color=\"blue\", linestyle=\"--\", marker=\"s\")\nplt.fill_between(range(len(actual_values)), actual_values, predicted_values, color=\"gray\", alpha=0.3, label=\"Difference\")\nplt.title(\"Energy Forecasting: Actual vs Predicted (Hybrid Model)\", fontsize=16, fontweight=\"bold\")\nplt.xlabel(\"Time Steps\", fontsize=14)\nplt.ylabel(\"Normalized Energy Consumption\", fontsize=14)\nplt.legend(fontsize=12, loc=\"best\", title=\"Legend\", title_fontsize=13)\nplt.grid(True, linestyle=\"--\", alpha=0.6)\nplt.tight_layout()\nplt.savefig(\"hybrid_forecasting_shading.png\")\nplt.savefig(\"hybrid_forecasting_shading.pdf\")\nplt.savefig(\"hybrid_forecasting_shading.eps\")\nplt.show()\n\n# ===========================\n# 7. Save the Performance Metrics for Later Comparison\n# ===========================\nnp.savez(\"hybrid_metrics.npz\",\n         epochs=np.array(metrics_history[\"epoch\"]),\n         loss=np.array(metrics_history[\"loss\"]),\n         val_loss=np.array(metrics_history[\"val_loss\"]),\n         accuracy=np.array(metrics_history[\"accuracy\"]),\n         val_accuracy=np.array(metrics_history[\"val_accuracy\"]),\n         epoch_times=np.array(epoch_times),\n         forecast_pred=np.array(predicted_values),\n         actual=np.array(actual_values))\nprint(\"Hybrid model metrics saved to hybrid_metrics.npz\")\n\"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport tensorflow as tf\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\nfrom collections import deque\nimport random\nimport time\n\n# ===========================\n# 1. Data Loading and Preprocessing\n# ===========================\ndata = pd.read_csv('/kaggle/input/integrated-energy-management-and-forecasting/Integrated Energy Management and Forecasting Dataset.csv')\n\n# Convert Timestamp to datetime and sort\ndata['Timestamp'] = pd.to_datetime(data['Timestamp'])\ndata.sort_values('Timestamp', inplace=True)\n\n# Select the 'Energy_Demand' feature and normalize it\nfeatures = ['Energy_Demand']\nscaler = MinMaxScaler()\ndata_features = scaler.fit_transform(data[features].values)\n\nsequence_length = 48\nprediction_length = 48\n\n# Create sequences and labels using a sliding window approach\nsequences, labels = [], []\nfor i in range(len(data_features) - sequence_length - prediction_length + 1):\n    seq = data_features[i:i + sequence_length]\n    label = data_features[i + sequence_length:i + sequence_length + prediction_length]\n    sequences.append(seq)\n    labels.append(label)\n\nsequences = np.array(sequences)\nlabels = np.array(labels).astype(np.float32)\n\n# ===========================\n# 2. Replay Buffer for Incremental Learning\n# ===========================\nclass ReplayBuffer:\n    def __init__(self, max_size=1000):\n        self.buffer = deque(maxlen=max_size)\n\n    def add(self, sequence, label):\n        self.buffer.append((sequence, label))\n\n    def sample(self, batch_size):\n        if len(self.buffer) < batch_size:\n            return list(self.buffer)\n        else:\n            return random.sample(self.buffer, batch_size)\n\n# ===========================\n# 3. Define a Deeper Lightweight Model Architecture\n# ===========================\ndef build_lightweight_model(seq_len=sequence_length, pred_len=prediction_length, dropout=0.2, hidden_dim=90):\n    inputs = tf.keras.layers.Input(shape=(seq_len, len(features)))\n    \n    # --- TCN Block 1 ---\n    # Manual causal padding: pad on the left (kernel_size - 1)\n    kernel_size = 3\n    pad_size = kernel_size - 1\n    x = tf.keras.layers.ZeroPadding1D(padding=(pad_size, 0))(inputs)\n    x = tf.keras.layers.SeparableConv1D(filters=hidden_dim, kernel_size=kernel_size,\n                                        padding=\"valid\", activation='relu')(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dropout(dropout)(x)\n    \n    # --- TCN Block 2 (deeper block) ---\n    x = tf.keras.layers.ZeroPadding1D(padding=(pad_size, 0))(x)\n    x = tf.keras.layers.SeparableConv1D(filters=hidden_dim, kernel_size=kernel_size,\n                                        padding=\"valid\", activation='relu')(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dropout(dropout)(x)\n    \n    # --- Bidirectional GRU Layer ---\n    x = tf.keras.layers.Bidirectional(\n        tf.keras.layers.GRU(hidden_dim, dropout=dropout, return_sequences=True, kernel_initializer='orthogonal')\n    )(x)\n    \n    # For forecasting, select the first 'pred_len' time steps\n    x = x[:, :pred_len, :]\n    outputs = tf.keras.layers.Dense(1, activation='linear')(x)\n    \n    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n        loss='mse',\n        metrics=['mae', 'mse', tf.keras.metrics.MeanAbsolutePercentageError(name='mape')]\n    )\n    \n    return model\n\n# Build and visualize the deeper lightweight model\nlightweight_model = build_lightweight_model()\nlightweight_model.summary()\n\ntf.keras.utils.plot_model(\n    lightweight_model,\n    to_file=\"hybrid_model_tcn_gru_lstm_architecture.png\",\n    show_shapes=True,\n    show_layer_names=True\n)\n\n# ===========================\n# 4. Incremental Online Training Loop with Per-Epoch Metrics\n# ===========================\nchunk_batch_size = 20         # Number of new samples per update (online chunk)\nepochs_per_chunk = 150        # Epochs per update\nreplay_sample_size = 32       # Number of samples to draw from replay buffer per update\n\nmetrics_history = {\n    \"epoch\": [],\n    \"loss\": [],\n    \"val_loss\": [],\n    \"accuracy\": [],\n    \"val_accuracy\": []\n}\nepoch_times = []\n\nreplay_buffer = ReplayBuffer(max_size=5000)\nglobal_epoch = 0\n\nnum_chunks = int(np.ceil(len(sequences) / chunk_batch_size))\nprint(f\"Starting online incremental training over {num_chunks} chunks...\")\n\nfor chunk_idx in range(0, len(sequences), chunk_batch_size):\n    new_sequences = sequences[chunk_idx:chunk_idx + chunk_batch_size]\n    new_labels = labels[chunk_idx:chunk_idx + chunk_batch_size]\n    \n    for seq, lbl in zip(new_sequences, new_labels):\n        replay_buffer.add(seq, lbl)\n    \n    replay_samples = replay_buffer.sample(replay_sample_size)\n    replay_sequences, replay_labels = zip(*replay_samples)\n    replay_sequences = np.array(replay_sequences)\n    replay_labels = np.array(replay_labels)\n    \n    train_sequences = np.vstack((new_sequences, replay_sequences))\n    train_labels = np.vstack((new_labels, replay_labels))\n    \n    start_time_chunk = time.time()\n    history = lightweight_model.fit(\n        train_sequences, train_labels,\n        batch_size=chunk_batch_size,\n        epochs=epochs_per_chunk,\n        verbose=0,\n        validation_split=0.1\n    )\n    end_time_chunk = time.time()\n    \n    chunk_time = end_time_chunk - start_time_chunk\n    avg_epoch_time = chunk_time / epochs_per_chunk\n    for epoch in range(epochs_per_chunk):\n        global_epoch += 1\n        loss = history.history['loss'][epoch]\n        val_loss = history.history['val_loss'][epoch]\n        acc = 1 - loss\n        val_acc = 1 - val_loss\n        metrics_history[\"epoch\"].append(global_epoch)\n        metrics_history[\"loss\"].append(loss)\n        metrics_history[\"val_loss\"].append(val_loss)\n        metrics_history[\"accuracy\"].append(acc)\n        metrics_history[\"val_accuracy\"].append(val_acc)\n        epoch_times.append(avg_epoch_time)\n    \n    print(f\"Chunk {chunk_idx//chunk_batch_size + 1}/{num_chunks}: \"\n          f\"Avg Loss={np.mean(history.history['loss']):.4f}, \"\n          f\"Avg Val Loss={np.mean(history.history['val_loss']):.4f}, \"\n          f\"Time for chunk={chunk_time:.2f}s\")\n\nmodel_save_path = \"hybrid_model_tcn_gru_lstm.h5\"\nlightweight_model.save(model_save_path)\nprint(f\"Model saved to {model_save_path}\")\n\n# ===========================\n# 5. Visualization of Performance Metrics per Epoch\n# ===========================\nplt.figure(figsize=(10, 6))\nplt.plot(metrics_history[\"epoch\"], metrics_history[\"loss\"], label=\"Training Loss\", marker=\"o\")\nplt.plot(metrics_history[\"epoch\"], metrics_history[\"val_loss\"], label=\"Validation Loss\", marker=\"s\", linestyle=\"--\")\nplt.title(\"Training and Validation Loss per Epoch\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.grid(True, linestyle=\"--\", alpha=0.6)\nplt.tight_layout()\nplt.savefig(\"hybrid_loss_per_epoch.png\")\nplt.savefig(\"hybrid_loss_per_epoch.pdf\")\nplt.savefig(\"hybrid_loss_per_epoch.eps\")\nplt.show()\n\nplt.figure(figsize=(10, 6))\nplt.plot(metrics_history[\"epoch\"], metrics_history[\"accuracy\"], label=\"Training Accuracy\", marker=\"o\")\nplt.plot(metrics_history[\"epoch\"], metrics_history[\"val_accuracy\"], label=\"Validation Accuracy\", marker=\"s\", linestyle=\"--\")\nplt.title(\"Training and Validation Accuracy per Epoch\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.grid(True, linestyle=\"--\", alpha=0.6)\nplt.tight_layout()\nplt.savefig(\"hybrid_accuracy_per_epoch.png\")\nplt.savefig(\"hybrid_accuracy_per_epoch.pdf\")\nplt.savefig(\"hybrid_accuracy_per_epoch.eps\")\nplt.show()\n\nplt.figure(figsize=(10, 6))\nplt.bar(metrics_history[\"epoch\"], epoch_times, color=\"blue\", alpha=0.7)\nplt.title(\"Training Time per Epoch\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Time (seconds)\")\nplt.grid(True, linestyle=\"--\", alpha=0.6)\nplt.tight_layout()\nplt.savefig(\"hybrid_time_per_epoch.png\")\nplt.savefig(\"hybrid_time_per_epoch.pdf\")\nplt.savefig(\"hybrid_time_per_epoch.eps\")\nplt.show()\n\n# ===========================\n# 6. Forecasting Visualization\n# ===========================\npredicted_values = lightweight_model.predict(np.expand_dims(sequences[-1], axis=0)).flatten()\nactual_values = labels[-1].flatten()\n\nplt.figure(figsize=(12, 8))\nplt.plot(range(len(actual_values)), actual_values, label=\"Actual Energy Consumption\", color=\"black\", marker=\"o\")\nplt.plot(range(len(predicted_values)), predicted_values, label=\"Hybrid Model Forecast\", color=\"blue\", linestyle=\"--\", marker=\"s\")\nplt.fill_between(range(len(actual_values)), actual_values, predicted_values, color=\"gray\", alpha=0.3, label=\"Difference\")\nplt.title(\"Energy Forecasting: Actual vs Predicted (Hybrid Model)\", fontsize=16, fontweight=\"bold\")\nplt.xlabel(\"Time Steps\", fontsize=14)\nplt.ylabel(\"Normalized Energy Consumption\", fontsize=14)\nplt.legend(fontsize=12, loc=\"best\", title=\"Legend\", title_fontsize=13)\nplt.grid(True, linestyle=\"--\", alpha=0.6)\nplt.tight_layout()\nplt.savefig(\"hybrid_forecasting_shading.png\")\nplt.savefig(\"hybrid_forecasting_shading.pdf\")\nplt.savefig(\"hybrid_forecasting_shading.eps\")\nplt.show()\n\n# ===========================\n# 7. Save the Performance Metrics for Later Comparison\n# ===========================\nnp.savez(\"hybrid_metrics.npz\",\n         epochs=np.array(metrics_history[\"epoch\"]),\n         loss=np.array(metrics_history[\"loss\"]),\n         val_loss=np.array(metrics_history[\"val_loss\"]),\n         accuracy=np.array(metrics_history[\"accuracy\"]),\n         val_accuracy=np.array(metrics_history[\"val_accuracy\"]),\n         epoch_times=np.array(epoch_times),\n         forecast_pred=np.array(predicted_values),\n         actual=np.array(actual_values))\nprint(\"Hybrid model metrics saved to hybrid_metrics.npz\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install graphviz\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T20:26:16.355794Z","iopub.execute_input":"2025-02-17T20:26:16.356131Z","iopub.status.idle":"2025-02-17T20:26:19.603202Z","shell.execute_reply.started":"2025-02-17T20:26:16.356103Z","shell.execute_reply":"2025-02-17T20:26:19.602302Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (0.20.3)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"pip install visualkeras\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T08:46:49.469487Z","iopub.execute_input":"2025-02-18T08:46:49.469864Z","iopub.status.idle":"2025-02-18T08:46:53.972746Z","shell.execute_reply.started":"2025-02-18T08:46:49.469835Z","shell.execute_reply":"2025-02-18T08:46:53.971885Z"}},"outputs":[{"name":"stdout","text":"Collecting visualkeras\n  Downloading visualkeras-0.1.4-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from visualkeras) (11.0.0)\nRequirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.10/dist-packages (from visualkeras) (1.26.4)\nCollecting aggdraw>=1.3.11 (from visualkeras)\n  Downloading aggdraw-1.3.19-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (655 bytes)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.18.1->visualkeras) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.18.1->visualkeras) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.18.1->visualkeras) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.18.1->visualkeras) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.18.1->visualkeras) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.18.1->visualkeras) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.18.1->visualkeras) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.18.1->visualkeras) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.18.1->visualkeras) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.18.1->visualkeras) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.18.1->visualkeras) (2024.2.0)\nDownloading visualkeras-0.1.4-py3-none-any.whl (17 kB)\nDownloading aggdraw-1.3.19-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (993 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m993.7/993.7 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: aggdraw, visualkeras\nSuccessfully installed aggdraw-1.3.19 visualkeras-0.1.4\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install pyvis\n!pip install Jinja2==2.11.3\n!pip install markupsafe==2.0.1\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T08:46:53.974135Z","iopub.execute_input":"2025-02-18T08:46:53.974459Z","iopub.status.idle":"2025-02-18T08:47:04.819099Z","shell.execute_reply.started":"2025-02-18T08:46:53.974432Z","shell.execute_reply":"2025-02-18T08:47:04.818072Z"}},"outputs":[{"name":"stdout","text":"Collecting pyvis\n  Downloading pyvis-0.3.2-py3-none-any.whl.metadata (1.7 kB)\nRequirement already satisfied: ipython>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from pyvis) (7.34.0)\nRequirement already satisfied: jinja2>=2.9.6 in /usr/local/lib/python3.10/dist-packages (from pyvis) (3.1.4)\nRequirement already satisfied: jsonpickle>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from pyvis) (4.0.1)\nRequirement already satisfied: networkx>=1.11 in /usr/local/lib/python3.10/dist-packages (from pyvis) (3.4.2)\nRequirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (75.1.0)\nRequirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (0.19.2)\nRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (4.4.2)\nRequirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (0.7.5)\nRequirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (5.7.1)\nRequirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (3.0.48)\nRequirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (2.18.0)\nRequirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (0.2.0)\nRequirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (0.1.7)\nRequirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (4.9.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.9.6->pyvis) (3.0.2)\nRequirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=5.3.0->pyvis) (0.8.4)\nRequirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=5.3.0->pyvis) (0.7.0)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.3.0->pyvis) (0.2.13)\nDownloading pyvis-0.3.2-py3-none-any.whl (756 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pyvis\nSuccessfully installed pyvis-0.3.2\nCollecting Jinja2==2.11.3\n  Downloading Jinja2-2.11.3-py2.py3-none-any.whl.metadata (3.5 kB)\nRequirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.10/dist-packages (from Jinja2==2.11.3) (3.0.2)\nDownloading Jinja2-2.11.3-py2.py3-none-any.whl (125 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.7/125.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: Jinja2\n  Attempting uninstall: Jinja2\n    Found existing installation: Jinja2 3.1.4\n    Uninstalling Jinja2-3.1.4:\n      Successfully uninstalled Jinja2-3.1.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbranca 0.8.1 requires jinja2>=3, but you have jinja2 2.11.3 which is incompatible.\neli5 0.13.0 requires jinja2>=3.0.0, but you have jinja2 2.11.3 which is incompatible.\nflask 3.1.0 requires Jinja2>=3.1.2, but you have jinja2 2.11.3 which is incompatible.\njupyterlab-server 2.27.3 requires jinja2>=3.0.3, but you have jinja2 2.11.3 which is incompatible.\nsphinx 8.1.3 requires Jinja2>=3.1, but you have jinja2 2.11.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed Jinja2-2.11.3\nCollecting markupsafe==2.0.1\n  Downloading MarkupSafe-2.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (3.2 kB)\nDownloading MarkupSafe-2.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (30 kB)\nInstalling collected packages: markupsafe\n  Attempting uninstall: markupsafe\n    Found existing installation: MarkupSafe 3.0.2\n    Uninstalling MarkupSafe-3.0.2:\n      Successfully uninstalled MarkupSafe-3.0.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbranca 0.8.1 requires jinja2>=3, but you have jinja2 2.11.3 which is incompatible.\neli5 0.13.0 requires jinja2>=3.0.0, but you have jinja2 2.11.3 which is incompatible.\nflask 3.1.0 requires Jinja2>=3.1.2, but you have jinja2 2.11.3 which is incompatible.\njupyterlab-server 2.27.3 requires jinja2>=3.0.3, but you have jinja2 2.11.3 which is incompatible.\nsphinx 8.1.3 requires Jinja2>=3.1, but you have jinja2 2.11.3 which is incompatible.\nwerkzeug 3.1.3 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed markupsafe-2.0.1\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip uninstall pyvis -y\n!pip install pyvis\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T08:47:09.145477Z","iopub.execute_input":"2025-02-18T08:47:09.145734Z","iopub.status.idle":"2025-02-18T08:47:13.446372Z","shell.execute_reply.started":"2025-02-18T08:47:09.145713Z","shell.execute_reply":"2025-02-18T08:47:13.445558Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: pyvis 0.3.2\nUninstalling pyvis-0.3.2:\n  Successfully uninstalled pyvis-0.3.2\nCollecting pyvis\n  Using cached pyvis-0.3.2-py3-none-any.whl.metadata (1.7 kB)\nRequirement already satisfied: ipython>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from pyvis) (7.34.0)\nRequirement already satisfied: jinja2>=2.9.6 in /usr/local/lib/python3.10/dist-packages (from pyvis) (2.11.3)\nRequirement already satisfied: jsonpickle>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from pyvis) (4.0.1)\nRequirement already satisfied: networkx>=1.11 in /usr/local/lib/python3.10/dist-packages (from pyvis) (3.4.2)\nRequirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (75.1.0)\nRequirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (0.19.2)\nRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (4.4.2)\nRequirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (0.7.5)\nRequirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (5.7.1)\nRequirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (3.0.48)\nRequirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (2.18.0)\nRequirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (0.2.0)\nRequirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (0.1.7)\nRequirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (4.9.0)\nRequirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.9.6->pyvis) (2.0.1)\nRequirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=5.3.0->pyvis) (0.8.4)\nRequirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=5.3.0->pyvis) (0.7.0)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.3.0->pyvis) (0.2.13)\nUsing cached pyvis-0.3.2-py3-none-any.whl (756 kB)\nInstalling collected packages: pyvis\nSuccessfully installed pyvis-0.3.2\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from pyvis.network import Network","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T08:47:16.744006Z","iopub.execute_input":"2025-02-18T08:47:16.744296Z","iopub.status.idle":"2025-02-18T08:47:17.014504Z","shell.execute_reply.started":"2025-02-18T08:47:16.744273Z","shell.execute_reply":"2025-02-18T08:47:17.013796Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import graphviz\n\ndef visualize_model_diagram_for_double_column():\n    \"\"\"\n    Creates a static, publication-ready diagram of the hybrid deep online learning model,\n    optimized for double-column (two-column) research papers.\n    This version adjusts the spacing to slightly increase the width and decrease the height\n    without rotating the diagram.\n    Output is saved as PNG, EPS, and PDF files.\n    \"\"\"\n    dot = graphviz.Digraph(comment=\"Hybrid Deep Online Learning Model\", format=\"png\")\n    \n    # Keep a top-to-bottom layout. Adjust spacing:\n    # - Increase nodesep to widen the layout (horizontal spacing between nodes)\n    # - Decrease ranksep to reduce vertical space between ranks (height)\n    dot.attr(rankdir=\"TB\", nodesep=\"1.0\", ranksep=\"0.5\")\n    \n    # Set white background and node styles.\n    dot.attr(bgcolor=\"#FFFFFF\")\n    dot.attr(\"node\",\n             shape=\"box\",\n             style=\"rounded,filled\",\n             color=\"#4B4B4B\",\n             fontname=\"Helvetica\",\n             fontsize=\"10\")\n    \n    # ---------------------\n    # Define the nodes\n    # ---------------------\n    dot.node(\"Data\", \"Data Preprocessing\\n& Sequences\", fillcolor=\"#D6EAF8\")\n    dot.node(\"Replay\", \"Replay Buffer\\n(Incremental Storage)\", fillcolor=\"#D5F5E3\")\n    dot.node(\"TrainingLoop\", \"Online Incremental\\nTraining Loop\\n(Chunks, Epochs, etc.)\",\n             shape=\"ellipse\", fillcolor=\"#FDEBD0\")\n    dot.node(\"TCN1\", \"TCN Block 1\\n(SeparableConv1D + BN + Dropout)\", fillcolor=\"#FCF3CF\")\n    dot.node(\"TCN2\", \"TCN Block 2\\n(SeparableConv1D + BN + Dropout)\", fillcolor=\"#FCF3CF\")\n    dot.node(\"BiGRU\", \"Bidirectional GRU\\n(90 units, dropout=0.2)\", fillcolor=\"#FDEBD0\")\n    dot.node(\"Dense\", \"Dense Layer\\n(1 neuron, linear)\", fillcolor=\"#EBDEF0\")\n    dot.node(\"Output\", \"Forecast Output\", fillcolor=\"#F9EBEA\")\n    \n    # ---------------------\n    # Define the edges\n    # ---------------------\n    edge_attrs = {\n        \"color\": \"#7B7B7B\",\n        \"arrowhead\": \"normal\",\n        \"penwidth\": \"1.3\"\n    }\n    dot.edge(\"Data\", \"Replay\", label=\"store sequences\", **edge_attrs)\n    dot.edge(\"Data\", \"TrainingLoop\", label=\"new chunk\", **edge_attrs)\n    dot.edge(\"Replay\", \"TrainingLoop\", label=\"sample batch\", **edge_attrs)\n    dot.edge(\"TrainingLoop\", \"TCN1\", label=\"train/update weights\", **edge_attrs)\n    dot.edge(\"TCN1\", \"TCN2\", **edge_attrs)\n    dot.edge(\"TCN2\", \"BiGRU\", **edge_attrs)\n    dot.edge(\"BiGRU\", \"Dense\", **edge_attrs)\n    dot.edge(\"Dense\", \"Output\", label=\"predictions\", **edge_attrs)\n    \n    # ---------------------\n    # Render as PNG\n    # ---------------------\n    png_filename = dot.render(\"hybrid_deep_online_model_diagram\", format=\"png\", cleanup=True, view=False)\n    print(\"PNG saved to:\", png_filename)\n    \n    # ---------------------\n    # Render as EPS\n    # ---------------------\n    dot.format = \"eps\"\n    eps_filename = dot.render(\"hybrid_deep_online_model_diagram\", cleanup=True, view=False)\n    print(\"EPS saved to:\", eps_filename)\n    \n    # ---------------------\n    # Render as PDF\n    # ---------------------\n    dot.format = \"pdf\"\n    pdf_filename = dot.render(\"hybrid_deep_online_model_diagram\", cleanup=True, view=False)\n    print(\"PDF saved to:\", pdf_filename)\n\nif __name__ == \"__main__\":\n    visualize_model_diagram_for_double_column()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T08:53:20.428096Z","iopub.execute_input":"2025-02-18T08:53:20.428444Z","iopub.status.idle":"2025-02-18T08:53:20.525165Z","shell.execute_reply.started":"2025-02-18T08:53:20.428416Z","shell.execute_reply":"2025-02-18T08:53:20.524213Z"}},"outputs":[{"name":"stdout","text":"PNG saved to: hybrid_deep_online_model_diagram.png\nEPS saved to: hybrid_deep_online_model_diagram.eps\nPDF saved to: hybrid_deep_online_model_diagram.pdf\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"# 2. CNN Model Incremental Learning","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.preprocessing import MinMaxScaler\nfrom collections import deque\nimport random\nimport time\nimport matplotlib.pyplot as plt\n\n# -----------------------------------------------------------------------------\n# 1. Data Loading and Preprocessing\n# -----------------------------------------------------------------------------\ndata = pd.read_csv('/kaggle/input/integrated-energy-management-and-forecasting/Integrated Energy Management and Forecasting Dataset.csv')\ndata['Timestamp'] = pd.to_datetime(data['Timestamp'])\ndata.sort_values('Timestamp', inplace=True)\n\nfeatures = ['Energy_Demand']\nscaler = MinMaxScaler()\ndata_features = scaler.fit_transform(data[features].values)\n\nsequence_length = 48\nprediction_length = 48\n\n# Create sequences and labels using a sliding window approach\nsequences, labels = [], []\nfor i in range(len(data_features) - sequence_length - prediction_length + 1):\n    seq = data_features[i:i + sequence_length]\n    label = data_features[i + sequence_length:i + sequence_length + prediction_length]\n    sequences.append(seq)\n    labels.append(label)\n\nsequences = np.array(sequences)\nlabels = np.array(labels).astype(np.float32)\n\n# -----------------------------------------------------------------------------\n# 2. Replay Buffer for Incremental Learning\n# -----------------------------------------------------------------------------\nclass ReplayBuffer:\n    def __init__(self, max_size=1000):\n        self.buffer = deque(maxlen=max_size)\n    \n    def add(self, sequence, label):\n        self.buffer.append((sequence, label))\n    \n    def sample(self, batch_size):\n        # Return all samples if the buffer is not full\n        if len(self.buffer) < batch_size:\n            return list(self.buffer)\n        return random.sample(self.buffer, batch_size)\n\ndef incremental_learning_update(model, new_sequences, new_labels, replay_buffer, batch_size=32, epochs=1):\n    # Add new samples to the replay buffer.\n    for seq, lbl in zip(new_sequences, new_labels):\n        replay_buffer.add(seq, lbl)\n    \n    # Sample a batch from the replay buffer.\n    replay_samples = replay_buffer.sample(batch_size)\n    replay_sequences, replay_labels = zip(*replay_samples)\n    replay_sequences = np.array(replay_sequences)\n    replay_labels = np.array(replay_labels)\n    \n    # Combine new data with replayed samples.\n    combined_sequences = np.vstack((new_sequences, replay_sequences))\n    combined_labels = np.vstack((new_labels, replay_labels))\n    \n    # Incremental training on the combined data.\n    history = model.fit(combined_sequences, combined_labels,\n                        batch_size=batch_size,\n                        epochs=epochs,\n                        verbose=0,\n                        validation_split=0.1)\n    return history\n\n# -----------------------------------------------------------------------------\n# 3. CNN Model Definition\n# -----------------------------------------------------------------------------\ndef build_cnn_model(seq_len=sequence_length, pred_len=prediction_length, hidden_dim=128):\n    inputs = tf.keras.layers.Input(shape=(seq_len, len(features)))\n    \n    # Three convolutional layers\n    conv1 = tf.keras.layers.Conv1D(filters=hidden_dim, kernel_size=3, activation='relu', padding='same')(inputs)\n    conv2 = tf.keras.layers.Conv1D(filters=hidden_dim, kernel_size=3, activation='relu', padding='same')(conv1)\n    conv3 = tf.keras.layers.Conv1D(filters=hidden_dim, kernel_size=3, activation='relu', padding='same')(conv2)\n    \n    # Global average pooling to aggregate the features across time steps\n    pool = tf.keras.layers.GlobalAveragePooling1D()(conv3)\n    dense1 = tf.keras.layers.Dense(64, activation='relu')(pool)\n    \n    # Output layer produces 'prediction_length' outputs\n    outputs = tf.keras.layers.Dense(pred_len, activation='linear')(dense1)\n    \n    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n    model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.001),\n                  loss='mse',\n                  metrics=['mae', 'mse', tf.keras.metrics.MeanAbsolutePercentageError(name='mape')])\n    return model\n\n# Build and summarize the CNN model\ncnn_model = build_cnn_model()\ncnn_model.summary()\n\n# -----------------------------------------------------------------------------\n# 4. Incremental Online Training Loop with Per-Epoch Metrics for CNN Model\n# -----------------------------------------------------------------------------\n# Training configuration\nchunk_batch_size = 20         # New samples per update (online chunk)\nepochs_per_chunk = 150         # Number of epochs per update\nreplay_sample_size = 32       # Number of samples drawn from the replay buffer per update\n\n# Dictionaries to record per-epoch metrics and training times\ncnn_metrics_history = {\n    \"epoch\": [],\n    \"loss\": [],\n    \"val_loss\": [],\n    \"accuracy\": [],\n    \"val_accuracy\": []\n}\ncnn_epoch_times = []\n\nreplay_buffer_cnn = ReplayBuffer(max_size=1000)\nglobal_epoch = 0\n\nnum_chunks = int(np.ceil(len(sequences) / chunk_batch_size))\nprint(f\"Starting online incremental training for CNN model over {num_chunks} chunks...\")\n\nfor chunk_idx in range(0, len(sequences), chunk_batch_size):\n    # Get the new data chunk\n    new_sequences = sequences[chunk_idx:chunk_idx + chunk_batch_size]\n    new_labels = labels[chunk_idx:chunk_idx + chunk_batch_size]\n    \n    # Add new data to the replay buffer\n    for seq, lbl in zip(new_sequences, new_labels):\n        replay_buffer_cnn.add(seq, lbl)\n    \n    # Sample replay data for this update\n    replay_samples = replay_buffer_cnn.sample(replay_sample_size)\n    replay_sequences, replay_labels = zip(*replay_samples)\n    replay_sequences = np.array(replay_sequences)\n    replay_labels = np.array(replay_labels)\n    \n    # Combine new data with replayed samples to form a training mini-batch\n    train_sequences = np.vstack((new_sequences, replay_sequences))\n    train_labels = np.vstack((new_labels, replay_labels))\n    \n    # Train the model for a fixed number of epochs on the combined data (simulate online update)\n    start_time_chunk = time.time()\n    history = cnn_model.fit(\n        train_sequences, train_labels,\n        batch_size=chunk_batch_size,\n        epochs=epochs_per_chunk,\n        verbose=0,\n        validation_split=0.1\n    )\n    end_time_chunk = time.time()\n    \n    chunk_time = end_time_chunk - start_time_chunk\n    avg_epoch_time = chunk_time / epochs_per_chunk\n    \n    # Record per-epoch metrics for this chunk update\n    for epoch in range(epochs_per_chunk):\n        global_epoch += 1\n        loss = history.history['loss'][epoch]\n        val_loss = history.history['val_loss'][epoch]\n        # \"Accuracy\" is defined here as 1 - loss for visualization purposes.\n        acc = 1 - loss\n        val_acc = 1 - val_loss\n        cnn_metrics_history[\"epoch\"].append(global_epoch)\n        cnn_metrics_history[\"loss\"].append(loss)\n        cnn_metrics_history[\"val_loss\"].append(val_loss)\n        cnn_metrics_history[\"accuracy\"].append(acc)\n        cnn_metrics_history[\"val_accuracy\"].append(val_acc)\n        cnn_epoch_times.append(avg_epoch_time)\n    \n    print(f\"Chunk {chunk_idx//chunk_batch_size + 1}/{num_chunks}: \"\n          f\"Avg Loss={np.mean(history.history['loss']):.4f}, \"\n          f\"Avg Val Loss={np.mean(history.history['val_loss']):.4f}, \"\n          f\"Time for chunk={chunk_time:.2f}s\")\n\n# -----------------------------------------------------------------------------\n# 5. Visualization of Performance Metrics per Epoch for CNN Model\n# -----------------------------------------------------------------------------\nplt.figure(figsize=(10, 6))\nplt.plot(cnn_metrics_history[\"epoch\"], cnn_metrics_history[\"loss\"], label=\"Training Loss\", marker=\"o\")\nplt.plot(cnn_metrics_history[\"epoch\"], cnn_metrics_history[\"val_loss\"], label=\"Validation Loss\", marker=\"s\", linestyle=\"--\")\nplt.title(\"CNN Model: Training and Validation Loss per Epoch\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.grid(True, linestyle=\"--\", alpha=0.6)\nplt.tight_layout()\nplt.savefig(\"cnn_loss_per_epoch.png\")\nplt.show()\n\nplt.figure(figsize=(10, 6))\nplt.plot(cnn_metrics_history[\"epoch\"], cnn_metrics_history[\"accuracy\"], label=\"Training Accuracy\", marker=\"o\")\nplt.plot(cnn_metrics_history[\"epoch\"], cnn_metrics_history[\"val_accuracy\"], label=\"Validation Accuracy\", marker=\"s\", linestyle=\"--\")\nplt.title(\"CNN Model: Training and Validation Accuracy per Epoch\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.grid(True, linestyle=\"--\", alpha=0.6)\nplt.tight_layout()\nplt.savefig(\"cnn_accuracy_per_epoch.png\")\nplt.show()\n\nplt.figure(figsize=(10, 6))\nplt.bar(cnn_metrics_history[\"epoch\"], cnn_epoch_times, color=\"blue\", alpha=0.7)\nplt.title(\"CNN Model: Training Time per Epoch\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Time (seconds)\")\nplt.grid(True, linestyle=\"--\", alpha=0.6)\nplt.tight_layout()\nplt.savefig(\"cnn_time_per_epoch.png\")\nplt.show()\n\n# -----------------------------------------------------------------------------\n# 6. Forecasting Visualization: Compare Actual vs. Predicted Values for CNN Model\n# -----------------------------------------------------------------------------\n# Select the last sequence from the dataset and its corresponding actual labels\nlast_sequence = sequences[-1]\nactual_values = labels[-1].flatten()\n\n# Generate forecast using the trained CNN model.\n# Note: The CNN model output shape is (batch_size, prediction_length). We use the first (and only) sample.\npredicted_values = cnn_model.predict(np.expand_dims(last_sequence, axis=0)).flatten()\n\nplt.figure(figsize=(12, 8))\nplt.plot(range(len(actual_values)), actual_values, label=\"Actual Energy Consumption\", color=\"black\", marker=\"o\")\nplt.plot(range(len(predicted_values)), predicted_values, label=\"CNN Model Forecast\", color=\"blue\", linestyle=\"--\", marker=\"s\")\nplt.fill_between(range(len(actual_values)), actual_values, predicted_values, \n                 color=\"gray\", alpha=0.3, label=\"Difference\")\nplt.title(\"Energy Forecasting: Actual vs Predicted (CNN Model)\", fontsize=16, fontweight=\"bold\")\nplt.xlabel(\"Time Steps\", fontsize=14)\nplt.ylabel(\"Normalized Energy Consumption\", fontsize=14)\nplt.legend(fontsize=12, loc=\"best\", title=\"Legend\", title_fontsize=13)\nplt.grid(True, linestyle=\"--\", alpha=0.6)\nplt.tight_layout()\nplt.savefig(\"cnn_forecasting_actual_vs_predicted.png\")\nplt.show()\n\n# -----------------------------------------------------------------------------\n# 7. Save the Trained CNN Model and Performance Metrics for Later Comparison\n# -----------------------------------------------------------------------------\nmodel_save_path = \"cnn_model_incremental.h5\"\ncnn_model.save(model_save_path)\nprint(f\"CNN model saved to {model_save_path}\")\n\n# Save performance metrics to a file for later comparison\nnp.savez(\"cnn_metrics.npz\",\n         epochs=np.array(cnn_metrics_history[\"epoch\"]),\n         loss=np.array(cnn_metrics_history[\"loss\"]),\n         val_loss=np.array(cnn_metrics_history[\"val_loss\"]),\n         accuracy=np.array(cnn_metrics_history[\"accuracy\"]),\n         val_accuracy=np.array(cnn_metrics_history[\"val_accuracy\"]),\n         epoch_times=np.array(cnn_epoch_times),\n         forecast_pred=np.array(predicted_values),\n         actual=np.array(actual_values))\nprint(\"CNN model metrics saved to cnn_metrics.npz\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3. LSTM Modeling Incremental Learning","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.preprocessing import MinMaxScaler\nfrom collections import deque\nimport random\nimport time\nimport matplotlib.pyplot as plt\n\n# -----------------------------------------------------------------------------\n# 1. Data Loading and Preprocessing\n# -----------------------------------------------------------------------------\ndata = pd.read_csv('/kaggle/input/integrated-energy-management-and-forecasting/Integrated Energy Management and Forecasting Dataset.csv')\ndata['Timestamp'] = pd.to_datetime(data['Timestamp'])\ndata.sort_values('Timestamp', inplace=True)\n\nfeatures = ['Energy_Demand']\nscaler = MinMaxScaler()\ndata_features = scaler.fit_transform(data[features].values)\n\nsequence_length = 48\nprediction_length = 48\n\n# Create sequences and labels using a sliding window approach\nsequences, labels = [], []\nfor i in range(len(data_features) - sequence_length - prediction_length + 1):\n    seq = data_features[i:i + sequence_length]\n    label = data_features[i + sequence_length:i + sequence_length + prediction_length]\n    sequences.append(seq)\n    labels.append(label)\n\nsequences = np.array(sequences)\nlabels = np.array(labels).astype(np.float32)\n\n# -----------------------------------------------------------------------------\n# 2. Replay Buffer for Incremental Learning\n# -----------------------------------------------------------------------------\nclass ReplayBuffer:\n    def __init__(self, max_size=1000):\n        self.buffer = deque(maxlen=max_size)\n    \n    def add(self, sequence, label):\n        self.buffer.append((sequence, label))\n    \n    def sample(self, batch_size):\n        # Return all samples if the buffer is not full\n        if len(self.buffer) < batch_size:\n            return list(self.buffer)\n        return random.sample(self.buffer, batch_size)\n\ndef incremental_learning_update(model, new_sequences, new_labels, replay_buffer, batch_size=32, epochs=1):\n    # Add new samples to the replay buffer.\n    for seq, lbl in zip(new_sequences, new_labels):\n        replay_buffer.add(seq, lbl)\n    \n    # Sample a batch from the replay buffer.\n    replay_samples = replay_buffer.sample(batch_size)\n    replay_sequences, replay_labels = zip(*replay_samples)\n    replay_sequences = np.array(replay_sequences)\n    replay_labels = np.array(replay_labels)\n    \n    # Combine new data with the replayed samples.\n    combined_sequences = np.vstack((new_sequences, replay_sequences))\n    combined_labels = np.vstack((new_labels, replay_labels))\n    \n    # Incremental training on the combined data.\n    history = model.fit(combined_sequences, combined_labels,\n                        batch_size=batch_size,\n                        epochs=epochs,\n                        verbose=0,\n                        validation_split=0.1)\n    return history\n\n# -----------------------------------------------------------------------------\n# 3. Define the LSTM Model\n# -----------------------------------------------------------------------------\ndef build_lstm_model(seq_len=sequence_length, pred_len=prediction_length, hidden_dim=128):\n    inputs = tf.keras.layers.Input(shape=(seq_len, len(features)))\n    \n    # LSTM layers with dropout (architecture as provided)\n    lstm1 = tf.keras.layers.LSTM(hidden_dim, return_sequences=True, dropout=0.3)(inputs)\n    lstm2 = tf.keras.layers.LSTM(hidden_dim, return_sequences=True, dropout=0.3)(lstm1)\n    lstm3 = tf.keras.layers.LSTM(hidden_dim, return_sequences=False, dropout=0.3)(lstm2)\n    \n    # Dense layers for final prediction\n    dense1 = tf.keras.layers.Dense(64, activation='relu')(lstm3)\n    outputs = tf.keras.layers.Dense(pred_len, activation='linear')(dense1)\n    \n    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n    model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.001),\n                  loss='mse',\n                  metrics=['mae', 'mse', tf.keras.metrics.MeanAbsolutePercentageError(name='mape')])\n    return model\n\n# Build and summarize the LSTM model\nlstm_model = build_lstm_model()\nlstm_model.summary()\n\n# -----------------------------------------------------------------------------\n# 4. Incremental Online Training Loop with Per-Epoch Metrics for the LSTM Model\n# -----------------------------------------------------------------------------\n# Training configuration\nchunk_batch_size = 20        # Number of new samples per update (online chunk)\nepochs_per_chunk = 150       # Number of epochs per update\nreplay_sample_size = 32      # Number of samples drawn from the replay buffer per update\n\n# Dictionaries to record per-epoch metrics and training times\nlstm_metrics_history = {\n    \"epoch\": [],\n    \"loss\": [],\n    \"val_loss\": [],\n    \"accuracy\": [],\n    \"val_accuracy\": []\n}\nlstm_epoch_times = []\n\nreplay_buffer_lstm = ReplayBuffer(max_size=1000)\nglobal_epoch = 0\n\nnum_chunks = int(np.ceil(len(sequences) / chunk_batch_size))\nprint(f\"Starting online incremental training for LSTM model over {num_chunks} chunks...\")\n\nfor chunk_idx in range(0, len(sequences), chunk_batch_size):\n    # Get the new data chunk\n    new_sequences = sequences[chunk_idx:chunk_idx + chunk_batch_size]\n    new_labels = labels[chunk_idx:chunk_idx + chunk_batch_size]\n    \n    # Add new data to the replay buffer\n    for seq, lbl in zip(new_sequences, new_labels):\n        replay_buffer_lstm.add(seq, lbl)\n    \n    # Sample replay data for this update\n    replay_samples = replay_buffer_lstm.sample(replay_sample_size)\n    replay_sequences, replay_labels = zip(*replay_samples)\n    replay_sequences = np.array(replay_sequences)\n    replay_labels = np.array(replay_labels)\n    \n    # Combine new data with replayed samples to form a training mini-batch\n    train_sequences = np.vstack((new_sequences, replay_sequences))\n    train_labels = np.vstack((new_labels, replay_labels))\n    \n    # Train the model for a fixed number of epochs on the combined data (simulate online update)\n    start_time_chunk = time.time()\n    history = incremental_learning_update(\n        lstm_model,\n        new_sequences,\n        new_labels,\n        replay_buffer_lstm,\n        batch_size=chunk_batch_size,\n        epochs=epochs_per_chunk\n    )\n    end_time_chunk = time.time()\n    \n    chunk_time = end_time_chunk - start_time_chunk\n    avg_epoch_time = chunk_time / epochs_per_chunk\n    \n    # Record per-epoch metrics for this chunk update\n    for epoch in range(epochs_per_chunk):\n        global_epoch += 1\n        loss = history.history['loss'][epoch]\n        val_loss = history.history['val_loss'][epoch]\n        # \"Accuracy\" is defined here as 1 - loss for visualization purposes.\n        acc = 1 - loss\n        val_acc = 1 - val_loss\n        lstm_metrics_history[\"epoch\"].append(global_epoch)\n        lstm_metrics_history[\"loss\"].append(loss)\n        lstm_metrics_history[\"val_loss\"].append(val_loss)\n        lstm_metrics_history[\"accuracy\"].append(acc)\n        lstm_metrics_history[\"val_accuracy\"].append(val_acc)\n        lstm_epoch_times.append(avg_epoch_time)\n    \n    print(f\"Chunk {chunk_idx//chunk_batch_size + 1}/{num_chunks}: \"\n          f\"Avg Loss={np.mean(history.history['loss']):.4f}, \"\n          f\"Avg Val Loss={np.mean(history.history['val_loss']):.4f}, \"\n          f\"Time for chunk={chunk_time:.2f}s\")\n\n# -----------------------------------------------------------------------------\n# 5. Visualization of Performance Metrics per Epoch for the LSTM Model\n# -----------------------------------------------------------------------------\nplt.figure(figsize=(10, 6))\nplt.plot(lstm_metrics_history[\"epoch\"], lstm_metrics_history[\"loss\"], label=\"Training Loss\", marker=\"o\")\nplt.plot(lstm_metrics_history[\"epoch\"], lstm_metrics_history[\"val_loss\"], label=\"Validation Loss\", marker=\"s\", linestyle=\"--\")\nplt.title(\"LSTM Model: Training and Validation Loss per Epoch\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.grid(True, linestyle=\"--\", alpha=0.6)\nplt.tight_layout()\nplt.savefig(\"lstm_loss_per_epoch.png\")\nplt.show()\n\nplt.figure(figsize=(10, 6))\nplt.plot(lstm_metrics_history[\"epoch\"], lstm_metrics_history[\"accuracy\"], label=\"Training Accuracy\", marker=\"o\")\nplt.plot(lstm_metrics_history[\"epoch\"], lstm_metrics_history[\"val_accuracy\"], label=\"Validation Accuracy\", marker=\"s\", linestyle=\"--\")\nplt.title(\"LSTM Model: Training and Validation Accuracy per Epoch\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.grid(True, linestyle=\"--\", alpha=0.6)\nplt.tight_layout()\nplt.savefig(\"lstm_accuracy_per_epoch.png\")\nplt.show()\n\nplt.figure(figsize=(10, 6))\nplt.bar(lstm_metrics_history[\"epoch\"], lstm_epoch_times, color=\"blue\", alpha=0.7)\nplt.title(\"LSTM Model: Training Time per Epoch\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Time (seconds)\")\nplt.grid(True, linestyle=\"--\", alpha=0.6)\nplt.tight_layout()\nplt.savefig(\"lstm_time_per_epoch.png\")\nplt.show()\n\n# -----------------------------------------------------------------------------\n# 6. Forecasting Visualization: Compare Actual vs. Predicted Values for LSTM Model\n# -----------------------------------------------------------------------------\n# Select the last sequence from the dataset and its corresponding actual labels\nlast_sequence = sequences[-1]\nactual_values = labels[-1].flatten()\n\n# Generate forecast using the trained LSTM model.\n# Note: The model output shape is (batch_size, prediction_length). We use the first sample.\npredicted_values = lstm_model.predict(np.expand_dims(last_sequence, axis=0)).flatten()\n\nplt.figure(figsize=(12, 8))\nplt.plot(range(len(actual_values)), actual_values, label=\"Actual Energy Consumption\", color=\"black\", marker=\"o\")\nplt.plot(range(len(predicted_values)), predicted_values, label=\"LSTM Model Forecast\", color=\"blue\", linestyle=\"--\", marker=\"s\")\nplt.fill_between(range(len(actual_values)), actual_values, predicted_values, \n                 color=\"gray\", alpha=0.3, label=\"Difference\")\nplt.title(\"Energy Forecasting: Actual vs Predicted (LSTM Model)\", fontsize=16, fontweight=\"bold\")\nplt.xlabel(\"Time Steps\", fontsize=14)\nplt.ylabel(\"Normalized Energy Consumption\", fontsize=14)\nplt.legend(fontsize=12, loc=\"best\", title=\"Legend\", title_fontsize=13)\nplt.grid(True, linestyle=\"--\", alpha=0.6)\nplt.tight_layout()\nplt.savefig(\"lstm_forecasting_actual_vs_predicted.png\")\nplt.show()\n\n# -----------------------------------------------------------------------------\n# 7. Save the Trained LSTM Model and Performance Metrics for Later Comparison\n# -----------------------------------------------------------------------------\nlstm_model_save_path = \"lstm_model_incremental.h5\"\nlstm_model.save(lstm_model_save_path)\nprint(f\"LSTM model saved to {lstm_model_save_path}\")\n\n# Save performance metrics to a file for later comparison\nnp.savez(\"lstm_metrics.npz\",\n         epochs=np.array(lstm_metrics_history[\"epoch\"]),\n         loss=np.array(lstm_metrics_history[\"loss\"]),\n         val_loss=np.array(lstm_metrics_history[\"val_loss\"]),\n         accuracy=np.array(lstm_metrics_history[\"accuracy\"]),\n         val_accuracy=np.array(lstm_metrics_history[\"val_accuracy\"]),\n         epoch_times=np.array(lstm_epoch_times),\n         forecast_pred=np.array(predicted_values),\n         actual=np.array(actual_values))\nprint(\"LSTM model metrics saved to lstm_metrics.npz\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4. LSTM Model in Non-Incremental Fashion.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport time\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\n\n# -----------------------------------------------------------------------------\n# 1. Data Loading and Preprocessing\n# -----------------------------------------------------------------------------\ndata = pd.read_csv('/kaggle/input/integrated-energy-management-and-forecasting/Integrated Energy Management and Forecasting Dataset.csv')\n\n# Convert Timestamp to datetime and sort\ndata['Timestamp'] = pd.to_datetime(data['Timestamp'])\ndata.sort_values('Timestamp', inplace=True)\n\n# Use 'Energy_Demand' as the feature for prediction and normalize\nfeatures = ['Energy_Demand']\nscaler = MinMaxScaler()\ndata_features = scaler.fit_transform(data[features].values)\n\nsequence_length = 48\nprediction_length = 48\n\n# Create sequences and labels\nsequences, labels = [], []\nfor i in range(len(data_features) - sequence_length - prediction_length + 1):\n    seq = data_features[i:i + sequence_length]\n    label = data_features[i + sequence_length:i + sequence_length + prediction_length]\n    sequences.append(seq)\n    labels.append(label)\n\nsequences = np.array(sequences).astype(np.float32)\nlabels = np.array(labels).astype(np.float32)\n\n# Split into training and testing sets\nsplit_idx = int(len(sequences) * 0.8)\nx_train, x_test = sequences[:split_idx], sequences[split_idx:]\ny_train, y_test = labels[:split_idx], labels[split_idx:]\n\nprint(f\"Training sequences shape: {x_train.shape}\")\nprint(f\"Training labels shape: {y_train.shape}\")\nprint(f\"Testing sequences shape: {x_test.shape}\")\nprint(f\"Testing labels shape: {y_test.shape}\")\n\n# -----------------------------------------------------------------------------\n# 2. Define the LSTM Model\n# -----------------------------------------------------------------------------\ndef build_lstm_model(seq_len, pred_len, hidden_dim=128, feature_dim=1):\n    inputs = tf.keras.layers.Input(shape=(seq_len, feature_dim))\n\n    # LSTM layers\n    lstm1 = tf.keras.layers.LSTM(hidden_dim, return_sequences=True, dropout=0.3)(inputs)\n    lstm2 = tf.keras.layers.LSTM(hidden_dim, return_sequences=True, dropout=0.3)(lstm1)\n    lstm3 = tf.keras.layers.LSTM(hidden_dim, return_sequences=False, dropout=0.3)(lstm2)\n\n    # Dense layers\n    dense1 = tf.keras.layers.Dense(64, activation='relu')(lstm3)\n    outputs = tf.keras.layers.Dense(pred_len, activation='linear')(dense1)\n\n    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n    model.compile(\n        optimizer=tf.optimizers.Adam(learning_rate=0.001),\n        loss='mse',\n        metrics=['mae', 'mse', tf.keras.metrics.MeanAbsolutePercentageError(name='mape')]\n    )\n    return model\n\n# Build the model\nlstm_model_nonil = build_lstm_model(seq_len=sequence_length, pred_len=prediction_length, feature_dim=1)\nlstm_model_nonil.summary()\n\n# -----------------------------------------------------------------------------\n# 3. Initialize Metrics Storage\n# -----------------------------------------------------------------------------\ntraining_times = []\nlosses = []\nval_losses = []\navg_accuracy = []    # Using MAE as a proxy for \"accuracy\" (lower MAE is better)\nval_accuracy = []    # Using validation MAE as well\n\n# -----------------------------------------------------------------------------\n# 4. Training Loop and Metrics Collection (Non-Incremental)\n# -----------------------------------------------------------------------------\nbatch_size = 32\nepochs = 3000\nfor epoch in range(epochs):\n    print(f\"Starting epoch {epoch + 1}/{epochs}\")\n    epoch_start_time = time.time()\n    history = lstm_model_nonil.fit(\n        x_train,\n        y_train,\n        batch_size=batch_size,\n        epochs=1,  # Train one epoch at a time to capture metrics\n        validation_split=0.2,\n        shuffle=True,\n        verbose=1\n    )\n    epoch_end_time = time.time()\n\n    # Record training time for this epoch\n    training_times.append(epoch_end_time - epoch_start_time)\n\n    # Record loss and validation loss\n    losses.append(history.history['loss'][0])\n    val_losses.append(history.history['val_loss'][0])\n\n    # Record average accuracy and validation accuracy (using MAE as proxy)\n    avg_accuracy.append(history.history['mae'][0])\n    val_accuracy.append(history.history['val_mae'][0])\n\n# -----------------------------------------------------------------------------\n# 5. Save the Trained LSTM Model (Non-Incremental)\n# -----------------------------------------------------------------------------\nlstm_model_nonil_save_path = \"lstm_model_nonil.h5\"\nlstm_model_nonil.save(lstm_model_nonil_save_path)\nprint(f\"Model saved to {lstm_model_nonil_save_path}\")\n\n# -----------------------------------------------------------------------------\n# 6. Forecasting Visualization: Generate Final Forecast on Test Data\n# -----------------------------------------------------------------------------\n# For forecasting, we select the last sample from the test set.\nforecast_pred = lstm_model_nonil.predict(np.expand_dims(x_test[-1], axis=0)).flatten()\nactual = y_test[-1].flatten()\n\n# Plot the forecast\nplt.figure(figsize=(12, 8))\nplt.plot(range(len(actual)), actual, label=\"Actual Energy Demand\", color=\"black\", marker=\"o\")\nplt.plot(range(len(forecast_pred)), forecast_pred, label=\"LSTM Forecast (Non-Incremental)\", color=\"blue\", linestyle=\"--\", marker=\"s\")\nplt.fill_between(range(len(actual)), actual, forecast_pred, color=\"gray\", alpha=0.3, label=\"Difference\")\nplt.title(\"Energy Forecasting: Actual vs Predicted (LSTM Non-Incremental Model)\", fontsize=16, fontweight=\"bold\")\nplt.xlabel(\"Time Steps\", fontsize=14)\nplt.ylabel(\"Normalized Energy Consumption\", fontsize=14)\nplt.legend(fontsize=12, loc=\"best\", title=\"Legend\", title_fontsize=13)\nplt.grid(True, linestyle=\"--\", alpha=0.6)\nplt.tight_layout()\nplt.savefig(\"lstm_nonil_forecasting.png\")\nplt.show()\n\n# -----------------------------------------------------------------------------\n# 7. Save Performance Metrics for Later Comparison\n# -----------------------------------------------------------------------------\n# Create an array for epoch numbers (1 to epochs)\nepoch_numbers = np.arange(1, epochs + 1)\n\nnp.savez(\"lstm_nonil_metrics.npz\",\n         epochs=epoch_numbers,\n         loss=np.array(losses),\n         val_loss=np.array(val_losses),\n         accuracy=np.array(avg_accuracy),\n         val_accuracy=np.array(val_accuracy),\n         epoch_times=np.array(training_times),\n         forecast_pred=forecast_pred,\n         actual=actual)\nprint(\"LSTM Non-Incremental model metrics saved to lstm_nonil_metrics.npz\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**xLSTM Modeling Importing**","metadata":{}},{"cell_type":"code","source":"# Clone the XLSTM repository\n!git clone https://github.com/NX-AI/xlstm.git\n\n# Navigate to the xlstm directory\n%cd xlstm\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Install PyTorch\n!pip install torch torchvision torchaudio\n\n# Install additional dependencies\n!pip install numpy pandas scikit-learn matplotlib\n\n# Install the xLSTM package\n!pip install xlstm\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!pip install seaborn\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from xlstm.xlstm.blocks.mlstm.layer import mLSTMLayerConfig\nfrom xlstm.xlstm.blocks.slstm.layer import sLSTMLayerConfig\nfrom xlstm.xlstm.xlstm_block_stack import xLSTMBlockStack, xLSTMBlockStackConfig\nfrom xlstm.xlstm.blocks.mlstm.block import mLSTMBlockConfig\nfrom xlstm.xlstm.blocks.slstm.block import sLSTMBlockConfig\nfrom xlstm.xlstm.components.feedforward import FeedForwardConfig","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!apt-get install ninja-build -y\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ninja --version\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!nvcc --version\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!rm -rf /root/.cache/torch_extensions/\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nprint(torch.__version__)\nprint(torch.cuda.is_available())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install torchinfo\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"try:\n    from torchinfo import summary  # For model summary\nexcept ImportError:\n    !pip install torchinfo\n    from torchinfo import summary\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 5. xLSTM Incremental Learning Model","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, TensorDataset, Subset\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nimport time\nfrom collections import deque\nimport random\nimport matplotlib.pyplot as plt\nfrom torchinfo import summary\n\n# Import XLSTM components\nfrom xlstm.xlstm.blocks.slstm.layer import sLSTMLayerConfig\nfrom xlstm.xlstm.blocks.slstm.block import sLSTMBlockConfig\nfrom xlstm.xlstm.blocks.mlstm.layer import mLSTMLayerConfig\nfrom xlstm.xlstm.blocks.mlstm.block import mLSTMBlockConfig\nfrom xlstm.xlstm.xlstm_block_stack import xLSTMBlockStack, xLSTMBlockStackConfig\n\n# -----------------------------------------------------------------------------\n# 1. Data Loading and Preprocessing\n# -----------------------------------------------------------------------------\ndef load_energy_dataset(file_path, sequence_length=48, prediction_length=48):\n    data = pd.read_csv(file_path)\n    data['Datetime'] = pd.to_datetime(data['Timestamp'])\n    # Drop unneeded columns\n    data = data.drop(columns=['Timestamp', 'Weather_Condition_x', 'Weather_Condition_y'])\n    data = data.sort_values(by='Datetime').reset_index(drop=True)\n    numeric_columns = ['Energy_Demand', 'Energy_Supply', 'Temperature', 'Grid_Load',\n                       'Renewable_Source_Output', 'NonRenewable_Source_Output', 'Energy_Price']\n    data = data[['Datetime'] + numeric_columns]\n    data_hourly = data.set_index('Datetime').resample('h').mean().ffill()\n\n    # Scale the 'Energy_Demand' column\n    energy_demand = data_hourly['Energy_Demand'].values.reshape(-1, 1)\n    scaler = MinMaxScaler()\n    data_scaled = scaler.fit_transform(energy_demand)\n\n    # Prepare sequences:\n    # Each sample: X of shape (sequence_length, 1) and y of shape (prediction_length, 1)\n    X, y = [], []\n    for i in range(len(data_scaled) - sequence_length - prediction_length + 1):\n        X.append(data_scaled[i : i + sequence_length])\n        y.append(data_scaled[i + sequence_length : i + sequence_length + prediction_length])\n    X = torch.tensor(X, dtype=torch.float32)    # (num_samples, 48, 1)\n    # Squeeze last dimension so that y has shape (num_samples, 48)\n    y = torch.tensor(y, dtype=torch.float32).squeeze(-1)\n    return DataLoader(TensorDataset(X, y), batch_size=32, shuffle=True), scaler, data_hourly\n\n# -----------------------------------------------------------------------------\n# 2. Split Data into Chunks for Incremental Training\n# -----------------------------------------------------------------------------\ndef split_data_into_chunks(data_loader, chunk_size):\n    data_chunks = []\n    X, y = data_loader.dataset.tensors\n    for i in range(0, len(X), chunk_size):\n        chunk_X = X[i : i + chunk_size]\n        chunk_y = y[i : i + chunk_size]\n        if len(chunk_X) > 0:\n            data_chunks.append(DataLoader(TensorDataset(chunk_X, chunk_y), batch_size=32, shuffle=True))\n    return data_chunks\n\n# -----------------------------------------------------------------------------\n# 3. Define XLSTM Model\n# -----------------------------------------------------------------------------\nclass XLSTMModel(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size, slstm_blocks=1, mlstm_blocks=1):\n        \"\"\"\n        output_size is set equal to prediction_length (e.g., 48).\n        \"\"\"\n        super(XLSTMModel, self).__init__()\n        self.input_embedding = nn.Linear(input_size, hidden_size)\n        self.blocks = nn.ModuleList()\n        block_idx = 0\n        # Add sLSTM blocks first\n        while slstm_blocks > 0:\n            slstm_layer_config = sLSTMLayerConfig(\n                hidden_size=hidden_size,\n                num_heads=4,\n                num_states=4,\n                backend='cuda',\n                function='slstm',\n                dropout=0.0\n            )\n            slstm_block_config = sLSTMBlockConfig(\n                slstm=slstm_layer_config,\n                _num_blocks=1,\n                _block_idx=block_idx\n            )\n            self.blocks.append(\n                xLSTMBlockStack(config=xLSTMBlockStackConfig(\n                    slstm_block=slstm_block_config,\n                    num_blocks=1,\n                    context_length=48,\n                    embedding_dim=hidden_size,\n                    dropout=0.0,\n                    add_post_blocks_norm=False\n                ))\n            )\n            slstm_blocks -= 1\n            block_idx += 1\n        # Then add mLSTM blocks\n        while mlstm_blocks > 0:\n            mlstm_layer_config = mLSTMLayerConfig(\n                num_heads=4,\n                embedding_dim=hidden_size,\n                dropout=0.0\n            )\n            mlstm_block_config = mLSTMBlockConfig(\n                mlstm=mlstm_layer_config,\n                _num_blocks=1,\n                _block_idx=block_idx\n            )\n            self.blocks.append(\n                xLSTMBlockStack(config=xLSTMBlockStackConfig(\n                    mlstm_block=mlstm_block_config,\n                    num_blocks=1,\n                    context_length=48,\n                    embedding_dim=hidden_size,\n                    dropout=0.0,\n                    add_post_blocks_norm=False\n                ))\n            )\n            mlstm_blocks -= 1\n            block_idx += 1\n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        # x: (batch_size, context_length, input_size)\n        x = self.input_embedding(x)  # (batch_size, 48, hidden_size)\n        for block in self.blocks:\n            x = block(x)\n        # Use output from the final time step for prediction; final shape: (batch_size, output_size)\n        x = self.fc(x[:, -1, :])\n        return x\n\n# -----------------------------------------------------------------------------\n# 4. Incremental Training (with Training & Validation split per chunk)\n# -----------------------------------------------------------------------------\ndef train_one_epoch(model, train_loader, optimizer, criterion, device):\n    model.train()\n    total_loss = 0\n    for batch_X, batch_y in train_loader:\n        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n        outputs = model(batch_X)\n        loss = criterion(outputs, batch_y)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    return total_loss / len(train_loader)\n\ndef evaluate(model, val_loader, criterion, device):\n    model.eval()\n    total_loss = 0\n    with torch.no_grad():\n        for batch_X, batch_y in val_loader:\n            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n            outputs = model(batch_X)\n            loss = criterion(outputs, batch_y)\n            total_loss += loss.item()\n    return total_loss / len(val_loader) if len(val_loader) > 0 else 0\n\ndef split_train_val(chunk_loader, val_ratio=0.1):\n    dataset = chunk_loader.dataset\n    total_samples = len(dataset)\n    indices = list(range(total_samples))\n    split = int(val_ratio * total_samples)\n    # Ensure at least one validation sample if possible\n    if total_samples > 1 and split < 1:\n        split = 1\n    random.shuffle(indices)\n    val_indices = indices[:split]\n    train_indices = indices[split:]\n    # If no training samples remain, use the whole dataset for both\n    if len(train_indices) == 0:\n        train_indices = indices\n        val_indices = indices\n    train_subset = Subset(dataset, train_indices)\n    val_subset = Subset(dataset, val_indices)\n    train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n    val_loader = DataLoader(val_subset, batch_size=32, shuffle=False)\n    return train_loader, val_loader\n\n# -----------------------------------------------------------------------------\n# 5. Main Incremental Training Script for XLSTM Model\n# -----------------------------------------------------------------------------\nfile_path = '/kaggle/input/integrated-energy-management-and-forecasting/Integrated Energy Management and Forecasting Dataset.csv'\ndata_loader, scaler, hourly_data = load_energy_dataset(file_path, sequence_length=48, prediction_length=48)\nchunk_size = 20  # same as in other models\ndata_chunks = split_data_into_chunks(data_loader, chunk_size=chunk_size)\n\n# Model configuration\ninput_size = 1\nhidden_size = 64\noutput_size = 48  # prediction_length\nslstm_blocks = 1\nmlstm_blocks = 1\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodelxlstm = XLSTMModel(input_size, hidden_size, output_size,\n                         slstm_blocks=slstm_blocks, mlstm_blocks=mlstm_blocks).to(device)\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(modelxlstm.parameters(), lr=0.001)\n\n# Print model summary (using dummy input)\nprint(summary(modelxlstm, input_size=(32, 48, 1)))\n\n# Training configuration: epochs per chunk\nchunk_epochs = 150\n\n# Dictionaries to record per-epoch metrics (global epoch indexing)\nxlstm_metrics_history = {\n    \"epoch\": [],\n    \"loss\": [],\n    \"val_loss\": [],\n    \"accuracy\": [],\n    \"val_accuracy\": []\n}\nxlstm_epoch_times = []\nglobal_epoch = 0\nchunk_training_times = []\n\nprint(\"Starting incremental training on XLSTM model...\")\nfor i, chunk_loader in enumerate(data_chunks):\n    print(f\"Training on chunk {i + 1}/{len(data_chunks)}...\")\n    # Split current chunk into training and validation sets (90/10 split)\n    train_loader, val_loader = split_train_val(chunk_loader, val_ratio=0.1)\n    chunk_start = time.time()\n    # For each epoch in the current chunk update:\n    for epoch in range(chunk_epochs):\n        start_epoch = time.time()\n        train_loss = train_one_epoch(modelxlstm, train_loader, optimizer, criterion, device)\n        val_loss = evaluate(modelxlstm, val_loader, criterion, device)\n        end_epoch = time.time()\n        global_epoch += 1\n        xlstm_metrics_history[\"epoch\"].append(global_epoch)\n        xlstm_metrics_history[\"loss\"].append(train_loss)\n        xlstm_metrics_history[\"val_loss\"].append(val_loss)\n        # For visualization, define \"accuracy\" as (1 - loss)\n        xlstm_metrics_history[\"accuracy\"].append(1 - train_loss)\n        xlstm_metrics_history[\"val_accuracy\"].append(1 - val_loss)\n        xlstm_epoch_times.append(end_epoch - start_epoch)\n        if (epoch + 1) % 25 == 0:\n            print(f\"  Global Epoch {global_epoch}, Chunk Epoch {epoch + 1}/{chunk_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n    chunk_end = time.time()\n    chunk_training_times.append(chunk_end - chunk_start)\n    \n    # Optionally, evaluate on entire chunk\n    modelxlstm.eval()\n    all_preds, all_targets = [], []\n    with torch.no_grad():\n        for batch_X, batch_y in chunk_loader:\n            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n            preds = modelxlstm(batch_X).cpu().numpy()\n            all_preds.extend(preds)\n            all_targets.extend(batch_y.cpu().numpy())\n    mae = mean_absolute_error(all_targets, all_preds)\n    mse = mean_squared_error(all_targets, all_preds)\n    r2 = r2_score(all_targets, all_preds)\n    print(f\"Chunk {i + 1} Evaluation - MAE: {mae:.4f}, MSE: {mse:.4f}, R²: {r2:.4f}, Chunk Time: {chunk_training_times[-1]:.2f}s\")\n\nprint(\"Incremental training complete.\\n\")\n\n\nos.chdir('/kaggle/working/')\nimport os\nprint(os.getcwd())\n\n# -----------------------------------------------------------------------------\n# 6. Visualization: Plot Training & Validation Loss and Accuracy per Epoch, and Training Time\n# -----------------------------------------------------------------------------\nepochs = xlstm_metrics_history[\"epoch\"]\nplt.figure(figsize=(10, 6))\nplt.plot(epochs, xlstm_metrics_history[\"loss\"], label=\"Training Loss\", marker=\"o\")\nplt.plot(epochs, xlstm_metrics_history[\"val_loss\"], label=\"Validation Loss\", marker=\"s\", linestyle=\"--\")\nplt.title(\"XLSTM Model: Training and Validation Loss per Epoch\")\nplt.xlabel(\"Global Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.grid(True, linestyle=\"--\", alpha=0.6)\nplt.tight_layout()\nplt.savefig(\"xlstm_loss_per_epoch.png\")\nplt.show()\n\nplt.figure(figsize=(10, 6))\nplt.plot(epochs, xlstm_metrics_history[\"accuracy\"], label=\"Training Accuracy\", marker=\"o\")\nplt.plot(epochs, xlstm_metrics_history[\"val_accuracy\"], label=\"Validation Accuracy\", marker=\"s\", linestyle=\"--\")\nplt.title(\"XLSTM Model: Training and Validation Accuracy per Epoch\")\nplt.xlabel(\"Global Epoch\")\nplt.ylabel(\"Accuracy (1 - Loss)\")\nplt.legend()\nplt.grid(True, linestyle=\"--\", alpha=0.6)\nplt.tight_layout()\nplt.savefig(\"xlstm_accuracy_per_epoch.png\")\nplt.show()\n\nplt.figure(figsize=(10, 6))\nplt.bar(epochs, xlstm_epoch_times, color=\"blue\", alpha=0.7)\nplt.title(\"XLSTM Model: Training Time per Epoch\")\nplt.xlabel(\"Global Epoch\")\nplt.ylabel(\"Time (seconds)\")\nplt.grid(True, linestyle=\"--\", alpha=0.6)\nplt.tight_layout()\nplt.savefig(\"xlstm_time_per_epoch.png\")\nplt.show()\n\n# -----------------------------------------------------------------------------\n# 7. Forecasting Visualization: Compare Actual vs. Predicted Sequences\n# -----------------------------------------------------------------------------\n# Use the last available sequence from the full dataset for forecasting.\nX_all, y_all = data_loader.dataset.tensors\nlast_sequence = X_all[-1]  # shape: (48, 1)\nactual_values = y_all[-1].flatten().numpy()  # shape: (48,)\n\nmodelxlstm.eval()\nwith torch.no_grad():\n    predicted_values = modelxlstm(last_sequence.unsqueeze(0).to(device)).cpu().numpy().flatten()  # shape: (48,)\n\n# Inverse transform the predicted and actual values to original scale.\npredicted_values_orig = scaler.inverse_transform(predicted_values.reshape(-1, 1)).flatten()\nactual_values_orig = scaler.inverse_transform(actual_values.reshape(-1, 1)).flatten()\n\nplt.figure(figsize=(12, 8))\nplt.plot(range(len(actual_values_orig)), actual_values_orig, label=\"Actual Energy Demand\", color=\"black\", marker=\"o\")\nplt.plot(range(len(predicted_values_orig)), predicted_values_orig, label=\"XLSTM Forecast\", color=\"blue\", linestyle=\"--\", marker=\"s\")\nplt.fill_between(range(len(actual_values_orig)), actual_values_orig, predicted_values_orig, color=\"gray\", alpha=0.3, label=\"Difference\")\nplt.title(\"XLSTM Model: Actual vs. Predicted Energy Demand\", fontsize=16, fontweight=\"bold\")\nplt.xlabel(\"Time Steps\", fontsize=14)\nplt.ylabel(\"Energy Demand\", fontsize=14)\nplt.legend(fontsize=12, loc=\"best\", title=\"Legend\", title_fontsize=13)\nplt.grid(True, linestyle=\"--\", alpha=0.6)\nplt.tight_layout()\nplt.savefig(\"xlstm_forecasting_actual_vs_predicted.png\")\nplt.show()\n\n# -----------------------------------------------------------------------------\n# 8. Save the Trained XLSTM Model and Performance Metrics for Later Comparison\n# -----------------------------------------------------------------------------\nmodel_save_path = \"xlstm_model_incremental.pt\"\ntorch.save(modelxlstm.state_dict(), model_save_path)\nprint(f\"XLSTM model saved to {model_save_path}\")\n\n# Save performance metrics to a file for later comparison\nnp.savez(\"xlstm_metrics.npz\",\n         epochs=np.array(xlstm_metrics_history[\"epoch\"]),\n         loss=np.array(xlstm_metrics_history[\"loss\"]),\n         val_loss=np.array(xlstm_metrics_history[\"val_loss\"]),\n         accuracy=np.array(xlstm_metrics_history[\"accuracy\"]),\n         val_accuracy=np.array(xlstm_metrics_history[\"val_accuracy\"]),\n         epoch_times=np.array(xlstm_epoch_times),\n         forecast_pred=np.array(predicted_values),\n         actual=np.array(actual_values))\nprint(\"XLSTM model metrics saved to xlstm_metrics.npz\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Comparison of all models","metadata":{}},{"cell_type":"code","source":"#os.chdir('/kaggle/working/')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#os.chdir('/kaggle/working/')\n#import os\nprint(os.getcwd())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\n# ---------------------------\n# Load Metrics for Each Model\n# ---------------------------\nhybrid_data = np.load(\"hybrid_metrics.npz\")\ncnn_data    = np.load(\"cnn_metrics.npz\")\nlstm_data   = np.load(\"lstm_metrics.npz\")\nxlstm_data  = np.load(\"xlstm_metrics.npz\")\n\n# Extract variables for the Hybrid model\nhybrid_epochs      = hybrid_data[\"epochs\"]\nhybrid_loss        = hybrid_data[\"loss\"]\nhybrid_val_loss    = hybrid_data[\"val_loss\"]\nhybrid_accuracy    = hybrid_data[\"accuracy\"]\nhybrid_val_accuracy= hybrid_data[\"val_accuracy\"]\nhybrid_epoch_times = hybrid_data[\"epoch_times\"]\nhybrid_forecast    = hybrid_data[\"forecast_pred\"]\nhybrid_actual      = hybrid_data[\"actual\"]\n\n# Extract variables for the CNN model\ncnn_epochs         = cnn_data[\"epochs\"]\ncnn_loss           = cnn_data[\"loss\"]\ncnn_val_loss       = cnn_data[\"val_loss\"]\ncnn_accuracy       = cnn_data[\"accuracy\"]\ncnn_val_accuracy   = cnn_data[\"val_accuracy\"]\ncnn_epoch_times    = cnn_data[\"epoch_times\"]\ncnn_forecast       = cnn_data[\"forecast_pred\"]\ncnn_actual         = cnn_data[\"actual\"]\n\n# Extract variables for the LSTM model\nlstm_epochs        = lstm_data[\"epochs\"]\nlstm_loss          = lstm_data[\"loss\"]\nlstm_val_loss      = lstm_data[\"val_loss\"]\nlstm_accuracy      = lstm_data[\"accuracy\"]\nlstm_val_accuracy  = lstm_data[\"val_accuracy\"]\nlstm_epoch_times   = lstm_data[\"epoch_times\"]\nlstm_forecast      = lstm_data[\"forecast_pred\"]\nlstm_actual        = lstm_data[\"actual\"]\n\n# Extract variables for the XLSTM model\nxlstm_epochs       = xlstm_data[\"epochs\"]\nxlstm_loss         = xlstm_data[\"loss\"]\nxlstm_val_loss     = xlstm_data[\"val_loss\"]\nxlstm_accuracy     = xlstm_data[\"accuracy\"]\nxlstm_val_accuracy = xlstm_data[\"val_accuracy\"]\nxlstm_epoch_times  = xlstm_data[\"epoch_times\"]\nxlstm_forecast     = xlstm_data[\"forecast_pred\"]\nxlstm_actual       = xlstm_data[\"actual\"]\n\n# ---------------------------\n# Create Subplots for Comparison\n# ---------------------------\nfig, axs = plt.subplots(2, 2, figsize=(16, 14))\n\n# --- Subplot 1: Training and Validation Loss vs. Epoch ---\naxs[0, 0].plot(hybrid_epochs, hybrid_loss, label='Hybrid Train Loss', color='blue')\naxs[0, 0].plot(hybrid_epochs, hybrid_val_loss, label='Hybrid Val Loss', color='blue', linestyle='--')\n\naxs[0, 0].plot(cnn_epochs, cnn_loss, label='CNN Train Loss', color='green')\naxs[0, 0].plot(cnn_epochs, cnn_val_loss, label='CNN Val Loss', color='green', linestyle='--')\n\naxs[0, 0].plot(lstm_epochs, lstm_loss, label='LSTM Train Loss', color='red')\naxs[0, 0].plot(lstm_epochs, lstm_val_loss, label='LSTM Val Loss', color='red', linestyle='--')\n\naxs[0, 0].plot(xlstm_epochs, xlstm_loss, label='XLSTM Train Loss', color='purple')\naxs[0, 0].plot(xlstm_epochs, xlstm_val_loss, label='XLSTM Val Loss', color='purple', linestyle='--')\n\naxs[0, 0].set_title('Training & Validation Loss')\naxs[0, 0].set_xlabel('Global Epoch')\naxs[0, 0].set_ylabel('Loss')\naxs[0, 0].legend()\naxs[0, 0].grid(True, linestyle='--', alpha=0.6)\n\n# --- Subplot 2: Training and Validation Accuracy vs. Epoch ---\naxs[0, 1].plot(hybrid_epochs, hybrid_accuracy, label='Hybrid Train Accuracy', color='blue')\naxs[0, 1].plot(hybrid_epochs, hybrid_val_accuracy, label='Hybrid Val Accuracy', color='blue', linestyle='--')\n\naxs[0, 1].plot(cnn_epochs, cnn_accuracy, label='CNN Train Accuracy', color='green')\naxs[0, 1].plot(cnn_epochs, cnn_val_accuracy, label='CNN Val Accuracy', color='green', linestyle='--')\n\naxs[0, 1].plot(lstm_epochs, lstm_accuracy, label='LSTM Train Accuracy', color='red')\naxs[0, 1].plot(lstm_epochs, lstm_val_accuracy, label='LSTM Val Accuracy', color='red', linestyle='--')\n\naxs[0, 1].plot(xlstm_epochs, xlstm_accuracy, label='XLSTM Train Accuracy', color='purple')\naxs[0, 1].plot(xlstm_epochs, xlstm_val_accuracy, label='XLSTM Val Accuracy', color='purple', linestyle='--')\n\naxs[0, 1].set_title('Training & Validation Accuracy')\naxs[0, 1].set_xlabel('Global Epoch')\naxs[0, 1].set_ylabel('Accuracy (1 - Loss)')\naxs[0, 1].legend()\naxs[0, 1].grid(True, linestyle='--', alpha=0.6)\n\n# --- Subplot 3: Training Time per Epoch ---\naxs[1, 0].plot(hybrid_epochs, hybrid_epoch_times, label='Hybrid', color='blue')\naxs[1, 0].plot(cnn_epochs, cnn_epoch_times, label='CNN', color='green')\naxs[1, 0].plot(lstm_epochs, lstm_epoch_times, label='LSTM', color='red')\naxs[1, 0].plot(xlstm_epochs, xlstm_epoch_times, label='XLSTM', color='purple')\n\naxs[1, 0].set_title('Training Time per Epoch')\naxs[1, 0].set_xlabel('Global Epoch')\naxs[1, 0].set_ylabel('Time (seconds)')\naxs[1, 0].legend()\naxs[1, 0].grid(True, linestyle='--', alpha=0.6)\n\n# --- Subplot 4: Forecasting Comparison (48-step) ---\n# Here, we assume that the actual forecast target is the same across models.\n# (If different, you might average or pick one, since ideally they should match.)\ntime_steps = np.arange(48)\naxs[1, 1].plot(time_steps, hybrid_actual, label='Actual', color='black', marker='o')\naxs[1, 1].plot(time_steps, hybrid_forecast, label='Hybrid Forecast', color='blue', linestyle='--', marker='s')\naxs[1, 1].plot(time_steps, cnn_forecast, label='CNN Forecast', color='green', linestyle='--', marker='s')\naxs[1, 1].plot(time_steps, lstm_forecast, label='LSTM Forecast', color='red', linestyle='--', marker='s')\naxs[1, 1].plot(time_steps, xlstm_forecast, label='XLSTM Forecast', color='purple', linestyle='--', marker='s')\n\naxs[1, 1].set_title('48-step Forecasting Comparison')\naxs[1, 1].set_xlabel('Time Step')\naxs[1, 1].set_ylabel('Energy Demand')\naxs[1, 1].legend()\naxs[1, 1].grid(True, linestyle='--', alpha=0.6)\n\nplt.tight_layout()\nplt.savefig(\"model_comparison.png\")\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The above results show that after some time, incremental learning keeps learning and batch-based learning stops learning, and after a while it keeps the lines straight, which means the model doesn't have more capacity to learn new data points, but the streaming model also keeps the old training and learning new data points, which is a good gesture to the natural learning process.","metadata":{}},{"cell_type":"markdown","source":"# Comparison of 5 Models, including LSTM Non-Incremental","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\n# -----------------------------\n# Load Metrics for Each Model\n# -----------------------------\nhybrid_data    = np.load(\"hybrid_metrics.npz\")\ncnn_data       = np.load(\"cnn_metrics.npz\")\nlstm_data      = np.load(\"lstm_metrics.npz\")\nlstm_nonil_data= np.load(\"lstm_nonil_metrics.npz\")\nxlstm_data     = np.load(\"xlstm_metrics.npz\")\n\n# Extract arrays for the Hybrid TCN-GRU-LSTM model\nhybrid_epochs       = hybrid_data[\"epochs\"]\nhybrid_loss         = hybrid_data[\"loss\"]\nhybrid_val_loss     = hybrid_data[\"val_loss\"]\nhybrid_accuracy     = hybrid_data[\"accuracy\"]\nhybrid_val_accuracy = hybrid_data[\"val_accuracy\"]\nhybrid_epoch_times  = hybrid_data[\"epoch_times\"]\nhybrid_forecast     = hybrid_data[\"forecast_pred\"]\nhybrid_actual       = hybrid_data[\"actual\"]\n\n# Extract arrays for the CNN model\ncnn_epochs          = cnn_data[\"epochs\"]\ncnn_loss            = cnn_data[\"loss\"]\ncnn_val_loss        = cnn_data[\"val_loss\"]\ncnn_accuracy        = cnn_data[\"accuracy\"]\ncnn_val_accuracy    = cnn_data[\"val_accuracy\"]\ncnn_epoch_times     = cnn_data[\"epoch_times\"]\ncnn_forecast        = cnn_data[\"forecast_pred\"]\ncnn_actual          = cnn_data[\"actual\"]\n\n# Extract arrays for the Incremental LSTM model\nlstm_epochs         = lstm_data[\"epochs\"]\nlstm_loss           = lstm_data[\"loss\"]\nlstm_val_loss       = lstm_data[\"val_loss\"]\nlstm_accuracy       = lstm_data[\"accuracy\"]\nlstm_val_accuracy   = lstm_data[\"val_accuracy\"]\nlstm_epoch_times    = lstm_data[\"epoch_times\"]\nlstm_forecast       = lstm_data[\"forecast_pred\"]\nlstm_actual         = lstm_data[\"actual\"]\n\n# Extract arrays for the Non-Incremental LSTM model\nlstm_nonil_epochs         = lstm_nonil_data[\"epochs\"]\nlstm_nonil_loss           = lstm_nonil_data[\"loss\"]\nlstm_nonil_val_loss       = lstm_nonil_data[\"val_loss\"]\nlstm_nonil_accuracy       = lstm_nonil_data[\"accuracy\"]\nlstm_nonil_val_accuracy   = lstm_nonil_data[\"val_accuracy\"]\nlstm_nonil_epoch_times    = lstm_nonil_data[\"epoch_times\"]\nlstm_nonil_forecast       = lstm_nonil_data[\"forecast_pred\"]\nlstm_nonil_actual         = lstm_nonil_data[\"actual\"]\n\n# Extract arrays for the XLSTM model\nxlstm_epochs       = xlstm_data[\"epochs\"]\nxlstm_loss         = xlstm_data[\"loss\"]\nxlstm_val_loss     = xlstm_data[\"val_loss\"]\nxlstm_accuracy     = xlstm_data[\"accuracy\"]\nxlstm_val_accuracy = xlstm_data[\"val_accuracy\"]\nxlstm_epoch_times  = xlstm_data[\"epoch_times\"]\nxlstm_forecast     = xlstm_data[\"forecast_pred\"]\nxlstm_actual       = xlstm_data[\"actual\"]\n\n# For forecasting, we assume that the actual target is the same across models.\n# (If not, you can choose one of them. Here, we choose the Hybrid model's actual.)\nactual_forecast = hybrid_actual  # shape: (48,)\n\n# -----------------------------\n# Create Subplots for Comparison (3 rows x 2 columns)\n# -----------------------------\nfig, axs = plt.subplots(3, 2, figsize=(20, 18))\n\n# Subplot 1: Training Loss vs. Epoch\naxs[0, 0].plot(hybrid_epochs, hybrid_loss, label='Hybrid Train Loss', color='blue')\naxs[0, 0].plot(cnn_epochs, cnn_loss, label='CNN Train Loss', color='green')\naxs[0, 0].plot(lstm_epochs, lstm_loss, label='LSTM Incremental Train Loss', color='red')\naxs[0, 0].plot(lstm_nonil_epochs, lstm_nonil_loss, label='LSTM Non-Incremental Train Loss', color='orange')\naxs[0, 0].plot(xlstm_epochs, xlstm_loss, label='XLSTM Train Loss', color='purple')\naxs[0, 0].set_title('Training Loss vs. Epoch')\naxs[0, 0].set_xlabel('Global Epoch')\naxs[0, 0].set_ylabel('Loss')\naxs[0, 0].legend(fontsize=9)\naxs[0, 0].grid(True, linestyle='--', alpha=0.6)\n\n# Subplot 2: Validation Loss vs. Epoch\naxs[0, 1].plot(hybrid_epochs, hybrid_val_loss, label='Hybrid Val Loss', color='blue', linestyle='--')\naxs[0, 1].plot(cnn_epochs, cnn_val_loss, label='CNN Val Loss', color='green', linestyle='--')\naxs[0, 1].plot(lstm_epochs, lstm_val_loss, label='LSTM Incremental Val Loss', color='red', linestyle='--')\naxs[0, 1].plot(lstm_nonil_epochs, lstm_nonil_val_loss, label='LSTM Non-Incremental Val Loss', color='orange', linestyle='--')\naxs[0, 1].plot(xlstm_epochs, xlstm_val_loss, label='XLSTM Val Loss', color='purple', linestyle='--')\naxs[0, 1].set_title('Validation Loss vs. Epoch')\naxs[0, 1].set_xlabel('Global Epoch')\naxs[0, 1].set_ylabel('Loss')\naxs[0, 1].legend(fontsize=9)\naxs[0, 1].grid(True, linestyle='--', alpha=0.6)\n\n# Subplot 3: Training Accuracy vs. Epoch\naxs[1, 0].plot(hybrid_epochs, hybrid_accuracy, label='Hybrid Train Acc', color='blue')\naxs[1, 0].plot(cnn_epochs, cnn_accuracy, label='CNN Train Acc', color='green')\naxs[1, 0].plot(lstm_epochs, lstm_accuracy, label='LSTM Incremental Train Acc', color='red')\naxs[1, 0].plot(lstm_nonil_epochs, lstm_nonil_accuracy, label='LSTM Non-Incremental Train Acc', color='orange')\naxs[1, 0].plot(xlstm_epochs, xlstm_accuracy, label='XLSTM Train Acc', color='purple')\naxs[1, 0].set_title('Training Accuracy (1 - Loss) vs. Epoch')\naxs[1, 0].set_xlabel('Global Epoch')\naxs[1, 0].set_ylabel('Accuracy')\naxs[1, 0].legend(fontsize=9)\naxs[1, 0].grid(True, linestyle='--', alpha=0.6)\n\n# Subplot 4: Validation Accuracy vs. Epoch\naxs[1, 1].plot(hybrid_epochs, hybrid_val_accuracy, label='Hybrid Val Acc', color='blue', linestyle='--')\naxs[1, 1].plot(cnn_epochs, cnn_val_accuracy, label='CNN Val Acc', color='green', linestyle='--')\naxs[1, 1].plot(lstm_epochs, lstm_val_accuracy, label='LSTM Incremental Val Acc', color='red', linestyle='--')\naxs[1, 1].plot(lstm_nonil_epochs, lstm_nonil_val_accuracy, label='LSTM Non-Incremental Val Acc', color='orange', linestyle='--')\naxs[1, 1].plot(xlstm_epochs, xlstm_val_accuracy, label='XLSTM Val Acc', color='purple', linestyle='--')\naxs[1, 1].set_title('Validation Accuracy (1 - Loss) vs. Epoch')\naxs[1, 1].set_xlabel('Global Epoch')\naxs[1, 1].set_ylabel('Accuracy')\naxs[1, 1].legend(fontsize=9)\naxs[1, 1].grid(True, linestyle='--', alpha=0.6)\n\n# Subplot 5: Training Time per Epoch\naxs[2, 0].plot(hybrid_epochs, hybrid_epoch_times, label='Hybrid', color='blue')\naxs[2, 0].plot(cnn_epochs, cnn_epoch_times, label='CNN', color='green')\naxs[2, 0].plot(lstm_epochs, lstm_epoch_times, label='LSTM Incremental', color='red')\naxs[2, 0].plot(lstm_nonil_epochs, lstm_nonil_epoch_times, label='LSTM Non-Incremental', color='orange')\naxs[2, 0].plot(xlstm_epochs, xlstm_epoch_times, label='XLSTM', color='purple')\naxs[2, 0].set_title('Training Time per Epoch')\naxs[2, 0].set_xlabel('Global Epoch')\naxs[2, 0].set_ylabel('Time (seconds)')\naxs[2, 0].legend(fontsize=9)\naxs[2, 0].grid(True, linestyle='--', alpha=0.6)\n\n# Subplot 6: Forecasting Comparison (48-step)\ntime_steps = np.arange(48)\naxs[2, 1].plot(time_steps, actual_forecast, label='Actual', color='black', marker='o')\naxs[2, 1].plot(time_steps, hybrid_forecast, label='Hybrid Forecast', color='blue', linestyle='--', marker='s')\naxs[2, 1].plot(time_steps, cnn_forecast, label='CNN Forecast', color='green', linestyle='--', marker='s')\naxs[2, 1].plot(time_steps, lstm_forecast, label='LSTM Incremental Forecast', color='red', linestyle='--', marker='s')\naxs[2, 1].plot(time_steps, lstm_nonil_forecast, label='LSTM Non-Incremental Forecast', color='orange', linestyle='--', marker='s')\naxs[2, 1].plot(time_steps, xlstm_forecast, label='XLSTM Forecast', color='purple', linestyle='--', marker='s')\naxs[2, 1].set_title('48-step Forecasting Comparison')\naxs[2, 1].set_xlabel('Time Step')\naxs[2, 1].set_ylabel('Energy Demand')\naxs[2, 1].legend(fontsize=9)\naxs[2, 1].grid(True, linestyle='--', alpha=0.6)\n\nplt.tight_layout()\nplt.savefig(\"model_comparison_all.png\")\nplt.savefig(\"model_comparison_all.eps\")\nplt.savefig(\"model_comparison_all.pdf\")\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport zipfile\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Create the output directory if it doesn't exist\noutput_dir = \"Comparsion_plotting\"\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n\n# -----------------------------\n# Load Metrics for Each Model\n# -----------------------------\nhybrid_data     = np.load(\"hybrid_metrics.npz\")\ncnn_data        = np.load(\"cnn_metrics.npz\")\nlstm_data       = np.load(\"lstm_metrics.npz\")\nlstm_nonil_data = np.load(\"lstm_nonil_metrics.npz\")\nxlstm_data      = np.load(\"xlstm_metrics.npz\")\n\n# Extract arrays for all models\nmodels = {\n    \"Hybrid\": hybrid_data,\n    \"CNN\": cnn_data,\n    \"LSTM_Incremental\": lstm_data,\n    \"LSTM_Non_Incremental\": lstm_nonil_data,\n    \"XLSTM\": xlstm_data\n}\n\nmetrics = [\"epochs\", \"loss\", \"val_loss\", \"accuracy\", \"val_accuracy\", \"epoch_times\", \"forecast_pred\", \"actual\"]\n\n# Extract all model data\nmodel_data = {\n    name: {metric: data[metric] for metric in metrics}\n    for name, data in models.items()\n}\n\n# Define plots to save individually\nplots = [\n    (\"training_loss_vs_epoch\", \"Training Loss vs. Epoch\", \"loss\", \"Loss\"),\n    (\"validation_loss_vs_epoch\", \"Validation Loss vs. Epoch\", \"val_loss\", \"Loss\"),\n    (\"training_accuracy_vs_epoch\", \"Training Accuracy vs. Epoch\", \"accuracy\", \"Accuracy\"),\n    (\"validation_accuracy_vs_epoch\", \"Validation Accuracy vs. Epoch\", \"val_accuracy\", \"Accuracy\"),\n    (\"training_time_per_epoch\", \"Training Time per Epoch\", \"epoch_times\", \"Time (seconds)\"),\n    (\"forecasting_comparison_all_models\", \"48-step Forecasting Comparison\", \"forecast_pred\", \"Energy Demand\")\n]\n\n# Define fixed color scheme\ncolor_scheme = {\n    \"Hybrid\": \"blue\",\n    \"CNN\": \"green\",\n    \"LSTM_Incremental\": \"red\",\n    \"LSTM_Non_Incremental\": \"orange\",\n    \"XLSTM\": \"purple\",\n    \"Actual\": \"black\"\n}\n\n# Loop through each plot type\nfor filename, title, metric, ylabel in plots:\n    \n    # Special handling for the forecast comparison plot\n    if metric == \"forecast_pred\":\n        # Create a figure and main axis\n        fig, ax = plt.subplots(figsize=(10, 6))\n        \n        # x-values for forecast (48 steps)\n        x_range = np.arange(48)\n\n        # 1) Plot Actual first (zorder=1) so it appears behind other curves\n        ax.plot(x_range,\n                model_data[\"Hybrid\"][\"actual\"],\n                label=\"Actual\",\n                color=color_scheme[\"Actual\"],\n                marker=\"o\",\n                markersize=5,\n                zorder=1)\n        \n        # 2) Plot other models, with Hybrid and XLSTM on top and XLSTM thinner\n        model_order = [\"Hybrid\", \"XLSTM\", \"CNN\", \"LSTM_Incremental\", \"LSTM_Non_Incremental\"]\n        for model_name in model_order:\n            z = 2\n            lw = 2.0  # default thickness\n            if model_name == \"Hybrid\":\n                z = 3\n            if model_name == \"XLSTM\":\n                z = 4\n                lw = 1.0  # make XLSTM thinner\n\n            ax.plot(x_range,\n                    model_data[model_name][\"forecast_pred\"],\n                    label=f\"{model_name} Forecast\",\n                    linestyle=\"--\",\n                    marker=\"s\",\n                    markersize=5,\n                    color=color_scheme[model_name],\n                    linewidth=lw,\n                    zorder=z)\n\n        # Basic labeling\n        ax.set_title(title)\n        ax.set_xlabel(\"Time Step\")\n        ax.set_ylabel(ylabel)\n        ax.legend(fontsize=9)\n        ax.grid(True, linestyle='--', alpha=0.6)\n\n        # Save the figure in PNG, PDF, and EPS formats\n        plt.savefig(os.path.join(output_dir, f\"{filename}.png\"), format=\"png\", dpi=300)\n        plt.savefig(os.path.join(output_dir, f\"{filename}.pdf\"), format=\"pdf\", dpi=300)\n        plt.savefig(os.path.join(output_dir, f\"{filename}.eps\"), format=\"eps\", dpi=300)\n        \n        plt.close()\n    \n    else:\n        # For the non-forecast plots, use your original approach\n        plt.figure(figsize=(10, 6))\n\n        for model_name, data in model_data.items():\n            plt.plot(data[\"epochs\"],\n                     data[metric],\n                     label=model_name,\n                     color=color_scheme[model_name])\n        \n        plt.title(title)\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(ylabel)\n        plt.legend(fontsize=9)\n        plt.grid(True, linestyle='--', alpha=0.6)\n\n        # Save the figure\n        plt.savefig(os.path.join(output_dir, f\"{filename}.png\"), format=\"png\", dpi=300)\n        plt.savefig(os.path.join(output_dir, f\"{filename}.pdf\"), format=\"pdf\", dpi=300)\n        plt.savefig(os.path.join(output_dir, f\"{filename}.eps\"), format=\"eps\", dpi=300)\n\n        plt.close()\n\nprint(\"All subplots saved individually in PNG, PDF, and EPS formats in the 'Comparsion_plotting' directory.\")\n\n# Create a zip file of the output directory\nzip_filename = \"Comparsion_plotting.zip\"\nwith zipfile.ZipFile(zip_filename, \"w\", zipfile.ZIP_DEFLATED) as zipf:\n    for foldername, subfolders, filenames in os.walk(output_dir):\n        for file in filenames:\n            file_path = os.path.join(foldername, file)\n            arcname = os.path.relpath(file_path, output_dir)\n            zipf.write(file_path, arcname=arcname)\n\nprint(f\"Zip file '{zip_filename}' created containing all plots.\")\n\n# If you're running this in a Jupyter Notebook, display a download link\ntry:\n    from IPython.display import FileLink, display\n    display(FileLink(zip_filename))\nexcept ImportError:\n    print(\"Run to display a download link.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install dataframe_image","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install nest_asyncio","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# -----------------------------\n# Load Metrics for Each Model (using absolute paths)\n# -----------------------------\nhybrid_data     = np.load(\"/kaggle/working/hybrid_metrics.npz\")\ncnn_data        = np.load(\"/kaggle/working/cnn_metrics.npz\")\nlstm_data       = np.load(\"/kaggle/working/lstm_metrics.npz\")\nlstm_nonil_data = np.load(\"/kaggle/working/lstm_nonil_metrics.npz\")\nxlstm_data      = np.load(\"/kaggle/working/xlstm_metrics.npz\")\n\n# Extract variables for the Hybrid TCN-GRU-LSTM model\nhybrid_epochs       = hybrid_data[\"epochs\"]\nhybrid_loss         = hybrid_data[\"loss\"]\nhybrid_val_loss     = hybrid_data[\"val_loss\"]\nhybrid_accuracy     = hybrid_data[\"accuracy\"]\nhybrid_val_accuracy = hybrid_data[\"val_accuracy\"]\nhybrid_epoch_times  = hybrid_data[\"epoch_times\"]\nhybrid_forecast     = hybrid_data[\"forecast_pred\"]\nhybrid_actual       = hybrid_data[\"actual\"]\n\n# Extract variables for the CNN model\ncnn_epochs          = cnn_data[\"epochs\"]\ncnn_loss            = cnn_data[\"loss\"]\ncnn_val_loss        = cnn_data[\"val_loss\"]\ncnn_accuracy        = cnn_data[\"accuracy\"]\ncnn_val_accuracy    = cnn_data[\"val_accuracy\"]\ncnn_epoch_times     = cnn_data[\"epoch_times\"]\ncnn_forecast        = cnn_data[\"forecast_pred\"]\ncnn_actual          = cnn_data[\"actual\"]\n\n# Extract variables for the Incremental LSTM model\nlstm_epochs         = lstm_data[\"epochs\"]\nlstm_loss           = lstm_data[\"loss\"]\nlstm_val_loss       = lstm_data[\"val_loss\"]\nlstm_accuracy       = lstm_data[\"accuracy\"]\nlstm_val_accuracy   = lstm_data[\"val_accuracy\"]\nlstm_epoch_times    = lstm_data[\"epoch_times\"]\nlstm_forecast       = lstm_data[\"forecast_pred\"]\nlstm_actual         = lstm_data[\"actual\"]\n\n# Extract variables for the Non-Incremental LSTM model\nlstm_nonil_epochs         = lstm_nonil_data[\"epochs\"]\nlstm_nonil_loss           = lstm_nonil_data[\"loss\"]\nlstm_nonil_val_loss       = lstm_nonil_data[\"val_loss\"]\nlstm_nonil_accuracy       = lstm_nonil_data[\"accuracy\"]\nlstm_nonil_val_accuracy   = lstm_nonil_data[\"val_accuracy\"]\nlstm_nonil_epoch_times    = lstm_nonil_data[\"epoch_times\"]\nlstm_nonil_forecast       = lstm_nonil_data[\"forecast_pred\"]\nlstm_nonil_actual         = lstm_nonil_data[\"actual\"]\n\n# Extract variables for the XLSTM model\nxlstm_epochs       = xlstm_data[\"epochs\"]\nxlstm_loss         = xlstm_data[\"loss\"]\nxlstm_val_loss     = xlstm_data[\"val_loss\"]\nxlstm_accuracy     = xlstm_data[\"accuracy\"]\nxlstm_val_accuracy = xlstm_data[\"val_accuracy\"]\nxlstm_epoch_times  = xlstm_data[\"epoch_times\"]\nxlstm_forecast     = xlstm_data[\"forecast_pred\"]\nxlstm_actual       = xlstm_data[\"actual\"]\n\n# For forecasting, we assume that the actual target is identical across models.\n# (If not, adjust accordingly; here we use Hybrid's actual.)\nactual_forecast = hybrid_actual  # shape: (48,)\n\n# -----------------------------\n# Compute Mean Metrics for Each Model\n# -----------------------------\ndef forecast_mae(forecast, actual):\n    return np.mean(np.abs(forecast - actual))\n\nhybrid_mean_train_loss = np.mean(hybrid_loss)\nhybrid_mean_val_loss   = np.mean(hybrid_val_loss)\nhybrid_mean_train_acc  = np.mean(hybrid_accuracy)\nhybrid_mean_val_acc    = np.mean(hybrid_val_accuracy)\nhybrid_mean_time       = np.mean(hybrid_epoch_times)\nhybrid_forecast_mae    = forecast_mae(hybrid_forecast, hybrid_actual)\n\ncnn_mean_train_loss = np.mean(cnn_loss)\ncnn_mean_val_loss   = np.mean(cnn_val_loss)\ncnn_mean_train_acc  = np.mean(cnn_accuracy)\ncnn_mean_val_acc    = np.mean(cnn_val_accuracy)\ncnn_mean_time       = np.mean(cnn_epoch_times)\ncnn_forecast_mae    = forecast_mae(cnn_forecast, cnn_actual)\n\nlstm_mean_train_loss = np.mean(lstm_loss)\nlstm_mean_val_loss   = np.mean(lstm_val_loss)\nlstm_mean_train_acc  = np.mean(lstm_accuracy)\nlstm_mean_val_acc    = np.mean(lstm_val_accuracy)\nlstm_mean_time       = np.mean(lstm_epoch_times)\nlstm_forecast_mae    = forecast_mae(lstm_forecast, lstm_actual)\n\nlstm_nonil_mean_train_loss = np.mean(lstm_nonil_loss)\nlstm_nonil_mean_val_loss   = np.mean(lstm_nonil_val_loss)\nlstm_nonil_mean_train_acc  = np.mean(lstm_nonil_accuracy)\nlstm_nonil_mean_val_acc    = np.mean(lstm_nonil_val_accuracy)\nlstm_nonil_mean_time       = np.mean(lstm_nonil_epoch_times)\nlstm_nonil_forecast_mae    = forecast_mae(lstm_nonil_forecast, lstm_nonil_actual)\n\nxlstm_mean_train_loss = np.mean(xlstm_loss)\nxlstm_mean_val_loss   = np.mean(xlstm_val_loss)\nxlstm_mean_train_acc  = np.mean(xlstm_accuracy)\nxlstm_mean_val_acc    = np.mean(xlstm_val_accuracy)\nxlstm_mean_time       = np.mean(xlstm_epoch_times)\nxlstm_forecast_mae    = forecast_mae(xlstm_forecast, xlstm_actual)\n\n# -----------------------------\n# Create a Comparison DataFrame\n# -----------------------------\ndf = pd.DataFrame({\n    \"Model\": [\"proposed iTGB-Net\", \"CNN\", \"LSTM Incremental\", \"LSTM Non-Incremental\", \"XLSTM\"],\n    \"Train Loss\": [hybrid_mean_train_loss, cnn_mean_train_loss, lstm_mean_train_loss, lstm_nonil_mean_train_loss, xlstm_mean_train_loss],\n    \"Val Loss\": [hybrid_mean_val_loss, cnn_mean_val_loss, lstm_mean_val_loss, lstm_nonil_mean_val_loss, xlstm_mean_val_loss],\n    \"Train Acc\": [hybrid_mean_train_acc, cnn_mean_train_acc, lstm_mean_train_acc, lstm_nonil_mean_train_acc, xlstm_mean_train_acc],\n    \"Val Acc\": [hybrid_mean_val_acc, cnn_mean_val_acc, lstm_mean_val_acc, lstm_nonil_mean_val_acc, xlstm_mean_val_acc],\n    \"Epoch Time (s)\": [hybrid_mean_time, cnn_mean_time, lstm_mean_time, lstm_nonil_mean_time, xlstm_mean_time],\n    \"Forecast MAE\": [hybrid_forecast_mae, cnn_forecast_mae, lstm_forecast_mae, lstm_nonil_forecast_mae, xlstm_forecast_mae]\n})\n\n# -----------------------------\n# Highlight Best Values Using LaTeX Bold Formatting\n# -----------------------------\n# For columns where lower is better: Train Loss, Val Loss, Epoch Time (s), Forecast MAE\n# For columns where higher is better: Train Acc, Val Acc\ndef format_cell(val, best, higher_is_better, decimals=6):\n    fmt_val = f\"{val:.{decimals}f}\"\n    if higher_is_better:\n        if np.isclose(val, best, atol=1e-6):\n            return r\"$\\mathbf{\" + fmt_val + \"}$\"\n    else:\n        if np.isclose(val, best, atol=1e-6):\n            return r\"$\\mathbf{\" + fmt_val + \"}$\"\n    return fmt_val\n\ndf_formatted = df.copy()\nfor col in df.columns:\n    if col == \"Model\":\n        continue\n    if col in [\"Train Acc\", \"Val Acc\"]:\n        best = df[col].max()\n        df_formatted[col] = df[col].apply(lambda x: format_cell(x, best, higher_is_better=True))\n    else:\n        best = df[col].min()\n        df_formatted[col] = df[col].apply(lambda x: format_cell(x, best, higher_is_better=False))\n\n# Create a 2D list for the table including headers\ntable_data = [df_formatted.columns.tolist()] + df_formatted.values.tolist()\n\n# -----------------------------\n# Create a Matplotlib Table and Save as PNG and EPS\n# -----------------------------\nfig, ax = plt.subplots(figsize=(12, 3))\nax.axis('tight')\nax.axis('off')\nthe_table = ax.table(cellText=table_data, colLabels=None, loc='center', cellLoc='center')\nthe_table.auto_set_font_size(False)\nthe_table.set_fontsize(10)\nfig.tight_layout()\nplt.savefig(\"/kaggle/working/model_comparison_table.png\", dpi=300)\nplt.savefig(\"/kaggle/working/model_comparison_table.eps\", format='eps')\nplt.show()\n\n# Also print the plain DataFrame (without LaTeX formatting) for reference\nprint(df)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}